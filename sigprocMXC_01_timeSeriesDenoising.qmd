---
title: Time series denoising
jupyter: python3
---

## Imports and signal generation

We will create a signal with a sampling rate of 1000Hz (1000 samples/s), and a
length of three seconds (@signal-gen). It will be created by linear interolation between some 
random time points $(p = 15)$.

```{python}
#| label: signal-gen
#| caption: "Imports and signal generation"
import numba as nb
import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import scipy.signal
from scipy.signal import windows as win
import scipy.stats as stats
from scipy import *
import copy

# set the seed
np.random.seed(42)

# create signal
srate = 1000 # Sampling rate (samples/sec, Hz.)
freq = 1/srate  # Frequency
time = np.arange(0, 3, freq) # three seconds total time
n = len(time)  # Sample count
p = 15 # poles for random interpolation

# amplitude modulator and noise level
signal_amp = 30
pure_signal = np.interp(
    x=np.linspace(0, p, n),  # interpolated x coordinate
    xp=np.arange(0, p),  # original x coordinates
    fp=np.random.rand(p) * signal_amp  # y coordinates
    )

# noise level (amplitude), measured in standard deviations
noise_amp = 5
noise  = noise_amp * np.random.randn(n)

# Add noise to the signal
noisy_signal = pure_signal + noise

plt.plot(noisy_signal)
plt.plot(pure_signal)
```

## VIDEO: Mean-smooth a time series

_Mean filter works by setting each data point in the filtered signal to be an
average of the surrounding points from the original signal._ @eq-mean-filter

$$
    y_t = \frac{1}{ 2k + 1 } \sum^{t+k}_{i=t-k} x_i
$$ {#eq-mean-filter}

_Now the number of data points that you go backwards in time and forwards in
time ($k$) is the key parameter of the mean smoothing filter, and that's
called the order of the filter._

_So each time point $t$ in the filtered signal $y$ is defined as the sum of all
the data points in the original signal $x$ going $t$ backwards in time, $k$ points
and $t$ forwards in time $k$ points._

_So here you sum all of these up and then you need to divide by the number of
time points, which is two K plus one._

_Here's where I implement the running mean filter in the time domain.
You will see in the section of this course on convolution that it's also
possible to implement this kind of filter in the frequency domain using
spectral multiplication. That's thanks to something called the convolution
theorem._

```{python}
#| label: mean-filter
#| caption: "Python implementation of the mean filter"

@nb.jit
def mean_filter(signal, radius):

    # initialize filtered signal vector
    n = len(signal)
    filtered = np.full(n, np.nan)

    # implement the running mean filter
    k = radius # filter window is actually (k * 2) + 1
    for i in range(k, n - k):
        # each point is the average of k surrounding points
        filtered[i] = np.mean(signal[i-k:i+k+1])
    
    return filtered

radius = 20
mean_filt = mean_filter(noisy_signal, radius=20)
```

_Now I've specified $k$ here in terms of the number of points, however, time
series data, people typically think about time in terms of milliseconds or
seconds or minutes or whatever is the relevant time scale, but some meaningful
time scale not necessarily points. And that's why I convert from points from
this arbitrary index number here into time in milliseconds._

_So this is the formula. Basically, this is saying the total number of points._

_So the total size of the window times 1000 and then divided by the sampling rate._

_And this will give you the window size in milliseconds._

_Now, in this particular case, it turns out that because I set the sampling rate
to be 1000, the window size is the same thing as the window size in points is
the same thing as the window size in milliseconds. But that's generally not the
case for sampling rates other than 1000._

```{python}
#| label: fig-run-mean
#| fig-cap: "plot the noisy and filtered signals"
# compute window size in milliseconds (ms)
window_diameter = 1000*(2 * radius+1) / srate

plt.plot(time, noisy_signal, lw=.1, label='Original')
plt.plot(time, mean_filt, color='k', label='filtered')

n = len(noisy_signal)
plt.legend()
plt.xlabel('Time (sec.)')
plt.ylabel('Amplitude')
plt.title(f'Running-mean filter with a k={window_diameter:.0f} millisecond window');
```

So there's a couple of interesting things that you can look at here.
First, it's interesting to see the signal still retains some roughness.
If we wanted the signalto be smoother we could increase $k$. At $k = 40$,
looks smoother, but the edges are still present.

Another thing you notice is some funny things happening at the start/end of the
signal called edge effects. If the filter is initialized to zero this will
create awkward jumps at the beginning and at the end.

Another possibility is to initialize the edges to be the original signal.
That doesn't give you this sudden drop down to zero, but it returns a kind of
bizarre looking, filtered signal where there's a lot of really high frequency
activity in the beginning and at the end.

### Closing remarks

So this is the running mean filter. 
It's a very useful filter, particularly when the noise is normally distributed.
This is not an appropriate filter for all kinds of noise.

This is really specific for when noise is distributed, positive and negative
relative to the signal of interest.

### Note: Edge effects

_In general, you always get something bizarre happening at the edges of your
time series Whenever you apply any kind of temporal filter. You will see
these edge effects every time you apply a temporal filter to data, regardless
of the type of filter. So then the question is what do you do with these edge points?_

* _Set them to be the original signal._
* _Ignore them by cropping off the initial/final data points from the filtered signal._

_Unfortunately, there's never a best or optimal way that always works for
dealing with edge effects. So usually you have to figure out what's the best
way to deal with edge effects on a case by case basis given your specific application._


## VIDEO: Gaussian-smooth a time series

Gaussian smoothing is very similar to Mean-smoothing, but it uses a Gaussian
weight instead of a constant weight of $1/n$.

$$
    y_t = \sum^{t+k}_{i=t-k} x_i g_i
$$ {#eq-gauss-filt}

The mean smoothing filter had fraction outside the the sum, dividing the point
count within the kernel ($n = 2k + 1$). The gaussian filter
uses a weighting function $g_i$, that encloses a total area of one.
Its formula typically is defined in terms of the mean ($\mu$) and stadard
deviation ($\sigma$, @eq-gauss-normal) 

$$
    g (x) =
    \frac{1}{\sigma \sqrt{2 \pi}}
    \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2}\right)
    = \frac{1}{\sigma \sqrt{2 \pi}} g_{pt}(t)
$$ {#eq-gauss-normal}

In signal processing the gaussian is defined based on the concept of the
full width at half maximum (@eq-fwhm). FWHM is the horizontal distance between the
sides of the distribution at its the vertical-mid point (50% gain). Ultimately
it reflects the same information as $\sigma$: how wide is the
distribution.

$$
    \mathrm{FWHM} = w = \sigma \sqrt{8 \ln 2}
$$ {#eq-fwhm}

If the kernel is centered at $0$, the formula in terms of FWHM becomes:

$$
    g (x)
    = \frac{2 \sqrt{\ln 2}}{w \sqrt{\pi}}
        \exp \left(  - \frac{4 \ln (2) x^2}{w^2} \right)
    = \frac{2 \sqrt{\ln 2}}{w \sqrt{\pi}} g_{pt} (t)
$$ {#eq-gauss-fwhm}

Gaussian filters can use the the gaussian distribution, or use a gaussian window
($g_{pt}$ in @eq-gauss-normal and @eq-gauss-fwhm).

$$
    \mathrm{window}
    = \exp \left( - \frac{1}{2} \left( \frac{n}{\sigma} \right)^2 \right)
    = \exp \left(  - \frac{4 \ln (2) x^2}{w^2} \right)
$$ {#eq-gauss-win}

A distribution has an area of one, whereas a window has a peak value of 1
(@fig-dist-vs-win red and blue lines, respectively).
This distinction is important because if the kernel is based on the the
distribution no normalization factor is needed. However, if the kernel is based
on the gaussian window it will need to be normalized by dividing by its sum.
Otherwise, the scale of the filtered signal will differ from the original.


```{python}
#| label: fig-dist-vs-win
#| caption: "Distribution vs. Window"
radius_samples = 50
diameter_samples =  2 * radius_samples + 1

mu_x, sigma_x = 0, 1
w_x = sigma_x * np.sqrt(8 * np.log(2))

radius_x = 3 * sigma_x
x = np.linspace(mu_x - radius_x, mu_x + radius_x, diameter_samples)

sigma_samples = 50/(radius_x/sigma_x)
x_samples =  np.arange(-radius_samples, radius_samples + 1) # "offset"

y = {}
part_sigma = 1/(sigma_x * np.sqrt(2 * np.pi))
y['$g(\mu, \sigma)$'] = part_sigma * np.exp(-((x - mu_x)**2)/(2 * sigma_x**2))
y['$g(\mu, \sigma)$ (ref.)'] = stats.norm.pdf(x, mu_x, sigma_x)

part_w = (2 * np.sqrt(np.log(2)))/(w_x * np.sqrt(np.pi))
y['$g(w)$'] = part_w * np.exp(- (4 * np.log(2) * x**2)/(w_x**2))

y['Win. $(\sigma)$'] = np.exp( -(1/2) * (x_samples/sigma_samples)**2 )
y['Win. $(\sigma)$ (ref.)'] = signal.windows.gaussian(diameter_samples, sigma_samples)
y['Win. $(w)$'] = np.exp( -(4 * np.log(2) * x**2) / (w_x**2) )
y[r'Win. $(\mu, \sigma) \rightarrow g(\sigma)$'] = part_w * y['Win. $(w)$']
y[r'Win. $(w) \rightarrow g(w)$'] = part_w * y['Win. $(w)$']

for z, (label, yvalues) in enumerate(y.items(), start=1):
    plt.plot(x, yvalues, label=label, zorder=-z)
plt.legend()
```

### An example kernel

We can illustrate the concept using right part of the formula $(g_{r})$ to
make a kernel with $w = 25$. On layman terms, this means we will apply 25 ms.
of smoothing, as most of the weight of the distribution is concetrated within
the FWHM. It is also necessary to define the radius of the
kernel $(k)$, whose length will bo $2k + 1$ time points. These two parameters
should be set ensuring the edges of the distribution defined by $w$
overlap the edges of the widow defined by $k$.

```{python}
#|label: gauss-window
#|caption: "Gaussian window"
@nb.jit
def make_gaussian(srate, radius, fwhm):
    k = radius
    positions = np.arange(-k, k + 1)
    time = 1000 * positions/srate
    gain = np.exp( -(4 * np.log(2) * time**2) / fwhm**2 )
    return time, gain
```

@fig-gauss-kernel shows three examples with a varying $k$. The first thing
we notice is that the empirical FWHM depicted on the legend is not exactly
the same. This is due several factors, with sampling rate playing the biggest
role (We do not have an infinite number points).

```{python}
#| label: fig-gauss-kernel
#| fig-cap: "Relationship between $k$ and FWHM"
def plot_gaussian_kernel(fwhm, kstyles):
    arrow_kwargs = {'arrowprops': {'arrowstyle': '<->'}}
    style = {'facecolor': 'w', 'edgecolor': 'k', 'boxstyle': 'round,pad=.2'}

    for k, color in kstyles.items():

        times, gains = make_gaussian(srate, radius=k, fwhm=fwhm)
        plt.axhline(.5, color='k', ls=':', zorder=0)
        plt.plot(times, gains, color='k') # Plot gaussian

        fwhm_0 = np.argmin( (gains[:k]-.5)**2 )
        fwhm_1 = np.argmin( (gains[k:]-.5)**2 ) + k # re-set indexing
        emp_fwhm = times[fwhm_1] - times[fwhm_0]
        plt.plot(
            [times[fwhm_0],times[fwhm_1]], [gains[fwhm_0],gains[fwhm_1]],
            'o-', label=f"k = {k}; Emp. FWHM = {emp_fwhm:.0f}") # Emp. FWHM
        
        start, end, gain0 = times[0], times[-1], gains[0] # Win. len
        plt.fill_between(x=times, y1=gain0, y2=gains, color=color)
        plt.annotate('', xy=(start, gain0), xytext=(end ,gain0), **arrow_kwargs)
        plt.text(0, gain0, f"k={k}", ha='center', va='center', bbox=style)

    plt.title(f"Gaussian kernel with target FWHM = {fwhm:.02f} ms")
    plt.xlabel('Time (ms)')
    plt.ylabel('Gain')
    plt.legend()

plot_gaussian_kernel(fwhm=25, kstyles={37: 'gray', 25: 'lightgray', 12: 'w'})
```

When $k = 12$, the kernel is a narrow window with a short time span. In fact,
it is too short, as the gain does not reach the bottom. The empirical FWHM
is shifted upwards, becoming the lowest point of the kernel, instead of the
vertical mid-point. $k = 25$ still is too low, whereas $k = 37$ is a reasonable
value, reaching down to near-zero vales. Note the kernel should not be too wide,
as the computational burden and the edge effects would increase, with very
little impact on the quality of the result. @fig-wide-gaussian shows a kernel
with $k = 100$ window, where most points will have very little impact, as points
past $\pm 30$ have  near-zero gains. Thus, it is good to have the smallest
possible $k$ that tapers down to $\approx 0$.

```{python}
#| label: fig-wide-gaussian
#| fig-cap: "Wide gaussian"
plot_gaussian_kernel(fwhm=25, kstyles={100: 'w', 25: 'lightgray'})
```

Note @fig-gauss-kernel and @fig-wide-gaussian both use $g_{pt}$ to generate the
kernels, which is not the complete formula of the gaussian. Thus, they are not
normalized $\left( \sum g \neq 1 \right)$. If applied, they would return a
filtered signal with a different scale than the original. To avoid this issue
is necessary to use the complete formula, or to normalize by dividing each gain
by the sum of all of them.

## The actual filtering

```{python}
#| label: fig-gaussian-filter
#| fig-cap: "Result of the gaussian filter"

@nb.jit
def gaussian_filter(signal, kernel):

    # initialize filtered signal vector
    filtered = signal.copy()

    # Calculate the kernel radius and the normalizer
    k = int((len(kernel) - 1)/2)
    normalizer = np.sum(kernel)
    
    # Apply the filter
    for i in range(k+1, n-k):
        # each point is the weighted average of k surrounding points
        filtered[i] = np.sum(signal[i-k:i+k+1] * kernel)/normalizer
    
    return filtered

_, kernel = make_gaussian(srate, radius=50, fwhm=25)
gauss_filt = gaussian_filter(noisy_signal, kernel)

# plot
plt.plot(time, noisy_signal,'gray', lw=.1, label='Original')
plt.plot(time, mean_filt, 'b', label='Mean-filtered')
plt.plot(time, gauss_filt,'r', label='Gaussian-filtered')

plt.xlabel('Time (s)')
# plt.xlim(1.7, 1.8)
plt.ylabel('amp. (a.u.)')
plt.legend()
plt.title('Gaussian smoothing filter');
```

The result of a gaussian filter tends to be smoother than those of its
mean-based counterpart, albeit the edges are even more roughen up.

The Gaussian filter  gives us a smoother time series compared to the running
mean filter. Which one is more useful is application-dependant. These two
filters are particularly useful when the noise is normally distributed around
the signal.

# VIDEO: Gaussian-smooth a spike time series

I'm going to show you the impact of applyng a Gaussian smoothing filter over
a spiky time series.

## The time series

The time series will contain 300 spikes, whose separation will be defined by
an exponential distribution, which is $e$ to the some random numbers.

An spike appears afer every 'space', looking like:

```{python}
## generate time series of random spikes

# number of spikes
spike_count = 300

# inter-spike intervals (exponential distribution for bursts)
spike_spaces = np.round(np.exp( np.random.randn(spike_count) )*10)

# generate spike time series
spiky_signal = np.zeros(int(sum(spike_spaces)))

for i in range(0, spike_count):
    spiky_signal[ int(np.sum(spike_spaces[0:i])) ] = 1


# plot
plt.plot(spiky_signal, lw=.1)
plt.xlabel('Time (a.u.)')
plt.show()
```

```{python}
## create and implement Gaussian window

# full-width half-maximum: the key Gaussian parameter
fwhm = 25 # in points

# normalized time vector in ms
k = 100;
gtime = np.arange(-k,k+1)

# create Gaussian window
gauswin = np.exp( -(4*np.log(2)*gtime**2) / fwhm**2 )
gauswin = gauswin / np.sum(gauswin)

# initialize filtered signal vector
filtsigG = np.zeros(len(spiky_signal))

# implement the weighted running mean filter
for i in range(k+1,len(spiky_signal)-k):
    filtsigG[i] = np.sum( spiky_signal[i-k:i+k+1]*gauswin )


# plot the filtered signal (spike probability density)
plt.plot(spiky_signal,'b',label='spikes')
plt.plot(filtsigG,'r',label='spike p.d.')
plt.legend()
plt.title('Spikes and spike probability density')
plt.show()
```

So then the idea is that each ISI, there's going to be another spike.

So let me show you what this looks like.

So here's the Spike Time series.

This is time just an arbitrary units.

And then you can see there's a series of spikes.

I'll zoom in so you can get a bit of a better look.

And what's interesting about these exponential distributions like this is that the spikes will have

this kind of bursty nature to them.

So you'll see spikes that happen in quick succession and relatively few spikes that are just kind of

hanging out all on their own.

So let me show you what this easy variable looks like so we can say plot easy.

Sorry.

Let me clear and then plot so you can see this is the duration of time between each successive spike.

And I can show you a histogram that might be a little clearer.

I say, let's say 30.

So you can see most spikes happen in quick succession.

There's a small time lag between successive spikes and the longer the time lag between individual spikes,

the less likely it is to see that long of an inter spike interval.

All right.

So let's go back to the plot here.

And now what I do is apply the Gaussian smoothing filter that you learned about previously.

In terms of the signal processing aspects in this video, there isn't a whole lot new.

It's more about seeing an additional application of something you just learned.

So here I define the full width at half maximum, the time vector for Gaussian.

Now, in a previous video I was multiplying this by 1000 and dividing by the sampling rate and that

was to convert this time vector into milliseconds.

Oops.

And this is actually wrong.

This is really just indices, little copy paste error there.

This shows why it's always important to check your code carefully when you copy and paste things anyway.

But fortunately that doesn't change the actual code.

It's just a little comment.

Here I define the Gaussian window, normalize the Gaussian to unit energy, and then loop through time

and take the sum of the Gaussian weighted data from the previous k time points to the following K time

points.

And then I plot the filtered signal.

So here's what that looks like.

It's the red line.

You can interpret this red line as being like a probability density.

So what is the probability of observing a spike at any given time point?

And that you get by essentially convolving the spike time series with the Gaussian window.

That's really what this is.

This is really convolution.

So when you can see what it's doing is just basically smoothing out the spike.

Time series like this.

So this red line could be more easily interpretable.

Maybe this is more amenable to certain data analyses.

And I just want to show you a few different options for the full width at half maximum and how that

affects the resulting image.

So if we set this to let's say I set it to five, run the code again.

Now you can see this hasn't really done a whole lot.

You know, the Gaussian now is so narrow, all it's really doing is like smoothing out each individual

spike.

It doesn't show much of an interesting time series.

In contrast, if I do, let's say 95, that's going to be too much.

So I think this is too much smoothing with 95 element full width at half maximum.

You can see this is mostly flat.

Other than extreme points here, like when there's no spikes here, you don't really see a whole lot

of interesting dynamics here.

So what exactly to set this parameter to is difficult to say a priori.

It depends on your specific application.

It depends on how much smoothing you think is appropriate for the data set that you are working with.

But the good thing to do is to try around several different smoothing parameters and see what the effects

are and what I like to do and what I always recommend when experimenting with different parameters for

analysis and filtering is to first define the extreme boundaries where the filter parameter doesn't

really make a lot of sense.

So that's why I showed you five indices and I showed you 95.

I think those are both extreme values that aren't really sensible for this data set.

And to me, 15 seems to look pretty good.

It still has a good amount of temporal precision while still showing some nice variability over time.

---
# VIDEO: Denoising via TKEO
---



```python
# import data
emgdata = sio.loadmat('TimeSeriesDenoising/emg4TKEO.mat')

# extract needed variables
emgtime = emgdata['emgtime'][0]
emg  = emgdata['emg'][0]

# initialize filtered signal
emgf = copy.deepcopy(emg)

# the loop version for interpretability
for i in range(1,len(emgf)-1):
    emgf[i] = emg[i]**2 - emg[i-1]*emg[i+1]

# the vectorized version for speed and elegance
emgf2 = copy.deepcopy(emg)
emgf2[1:-1] = emg[1:-1]**2 - emg[0:-2]*emg[2:]

## convert both signals to zscore

# find timepoint zero
time0 = np.argmin(emgtime**2)

# convert original EMG to z-score from time-zero
emgZ = (emg-np.mean(emg[0:time0])) / np.std(emg[0:time0])

# same for filtered EMG energy
emgZf = (emgf-np.mean(emgf[0:time0])) / np.std(emgf[0:time0])


## plot
# plot "raw" (normalized to max.1)
plt.plot(emgtime,emg/np.max(emg),'b',label='EMG')
plt.plot(emgtime,emgf/np.max(emgf),'m',label='TKEO energy')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude or energy')
plt.legend()

plt.show()

# plot zscored
plt.plot(emgtime,emgZ,'b',label='EMG')
plt.plot(emgtime,emgZf,'m',label='TKEO energy')

plt.xlabel('Time (ms)')
plt.ylabel('Zscore relative to pre-stimulus')
plt.legend()
plt.show()
```


---
VIDEO: Median filter to remove spike noise
---



```python
# create signal
n = 2000
signal = np.cumsum(np.random.randn(n))


# proportion of time points to replace with noise
propnoise = .05

# find noise points
noisepnts = np.random.permutation(n)
noisepnts = noisepnts[0:int(n*propnoise)]

# generate signal and replace points with noise
signal[noisepnts] = 50+np.random.rand(len(noisepnts))*100


# use hist to pick threshold
plt.hist(signal,100)
plt.show()

# visual-picked threshold
threshold = 40


# find data values above the threshold
suprathresh = np.where( signal>threshold )[0]

# initialize filtered signal
filtsig = copy.deepcopy(noisy_signal)

# loop through suprathreshold points and set to median of k
k = 20 # actual window is k*2+1
for ti in range(len(suprathresh)):
    
    # lower and upper bounds
    lowbnd = np.max((0,suprathresh[ti]-k))
    uppbnd = np.min((suprathresh[ti]+k+1,n))
    
    # compute median of surrounding points
    filtsig[suprathresh[ti]] = np.median(signal[lowbnd:uppbnd])

# plot
plt.plot(range(0,n),signal, range(0,n),filtsig)
plt.show()

```


---
# VIDEO: Remove linear trend
---



```python
# create signal with linear trend imposed
n = 2000
trended_signal = np.cumsum(np.random.randn(n)) + np.linspace(-30,30,n)

# linear detrending
detsignal = scipy.signal.detrend(trended_signal)

# get means
omean = np.mean(trended_signal) # original mean
dmean = np.mean(detsignal) # detrended mean

# plot signal and detrended signal
plt.plot(range(0,n),signal,label='Original, mean=%d' %omean)
plt.plot(range(0,n),detsignal,label='Detrended, mean=%d' %dmean)

plt.legend()
plt.show()
```


---
# VIDEO: Remove nonlinear trend with polynomials
---



```python
## polynomial intuition

order = 2
x = np.linspace(-15,15,100)

y = np.zeros(len(x))

for i in range(order+1):
    y = y + np.random.randn(1)*x**i

plt.plot(x,y)
plt.title('Order-%d polynomial' %order)
plt.show()

```


```python
## generate signal with slow polynomial artifact

n = 10000
t = range(n)
k = 10 # number of poles for random amplitudes

slowdrift = np.interp(np.linspace(1,k,n),np.arange(0,k),100*np.random.randn(k))
signal = slowdrift + 20*np.random.randn(n)

# plot
plt.plot(t,signal)
plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude')
plt.show()
```


```python
## fit a 3-order polynomial

# polynomial fit (returns coefficients)
p = np.polyfit(t,signal,3)

# predicted data is evaluation of polynomial
yHat = np.polyval(p,t)

# compute residual (the cleaned signal)
residual = signal - yHat


# now plot the fit (the function that will be removed)
plt.plot(t,signal,'b',label='Original')
plt.plot(t,yHat,'r',label='Polyfit')
plt.plot(t,residual,'k',label='Filtered signal')

plt.legend()
plt.show()
```


```python
## Bayes information criterion to find optimal order

# possible orders
orders = range(5,40)

# sum of squared errors (sse is reserved!)
sse1 = np.zeros(len(orders))

# loop through orders
for ri in range(len(orders)):
    
    # compute polynomial (fitting time series)
    yHat = np.polyval(np.polyfit(t,signal,orders[ri]),t)
    
    # compute fit of model to data (sum of squared errors)
    sse1[ri] = np.sum( (yHat-signal)**2 )/n


# Bayes information criterion
bic = n*np.log(sse1) + orders*np.log(n)

# best parameter has lowest BIC
bestP = min(bic)
idx = np.argmin(bic)

# plot the BIC
plt.plot(orders,bic,'ks-')
plt.plot(orders[idx],bestP,'ro')
plt.xlabel('Polynomial order')
plt.ylabel('Bayes information criterion')
plt.show()
```


```python
## now repeat filter for best (smallest) BIC

# polynomial fit
polycoefs = np.polyfit(t,signal,orders[idx])

# estimated data based on the coefficients
yHat = np.polyval(polycoefs,t)

# filtered signal is residual
filtsig = signal - yHat


## plotting
plt.plot(t,signal,'b',label='Original')
plt.plot(t,yHat,'r',label='Polynomial fit')
plt.plot(t,filtsig,'k',label='Filtered')

plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude')
plt.legend()
plt.show()
```


---
# VIDEO: Averaging multiple repetitions (time-synchronous averaging)
---



```python
## simulate data

# create event (derivative of Gaussian)
k = 100 # duration of event in time points
event = np.diff(np.exp( -np.linspace(-2,2,k+1)**2 ))
event = event/np.max(event) # normalize to max=1

# event onset times
Nevents = 30
onsettimes = np.random.permutation(10000-k)
onsettimes = onsettimes[0:Nevents]

# put event into data
data = np.zeros(10000)
for ei in range(Nevents):
    data[onsettimes[ei]:onsettimes[ei]+k] = event

# add noise
data = data + .5*np.random.randn(len(data))

# plot data
plt.subplot(211)
plt.plot(data)

# plot one event
plt.subplot(212)
plt.plot(range(k), data[onsettimes[3]:onsettimes[3]+k])
plt.plot(range(k), event)
plt.show()
```


```python
## extract all events into a matrix

datamatrix = np.zeros((Nevents,k))

for ei in range(Nevents):
    datamatrix[ei,:] = data[onsettimes[ei]:onsettimes[ei]+k]

plt.imshow(datamatrix)
plt.xlabel('Time')
plt.ylabel('Event number')
plt.title('All events')
plt.show()

plt.plot(range(0,k),np.mean(datamatrix,axis=0),label='Averaged')
plt.plot(range(0,k),event,label='Ground-truth')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.legend()
plt.title('Average events')
plt.show()
```


---
# VIDEO: Remove artifact via least-squares template-matching
---



```python
# load dataset
matdat = sio.loadmat('templateProjection.mat')
EEGdat = matdat['EEGdat']
eyedat = matdat['eyedat']
timevec = matdat['timevec'][0]
MN = np.shape(EEGdat) # matrix sizes

# initialize residual data
resdat = np.zeros(np.shape(EEGdat))


# loop over trials
for triali in range(MN[1]):
    
    # build the least-squares model as intercept and EOG from this trial
    X = np.column_stack((np.ones(MN[0]),eyedat[:,triali]))
    
    # compute regression coefficients for EEG channel
    b = np.linalg.solve(X.T@X,X.T@EEGdat[:,triali])
    
    # predicted data
    yHat = X@b
    
    # new data are the residuals after projecting out the best EKG fit
    resdat[:,triali] = EEGdat[:,triali] - yHat
```


```python
### plotting

# trial averages
plt.plot(timevec,np.mean(eyedat,axis=1),label='EOG')
plt.plot(timevec,np.mean(EEGdat,axis=1),label='EEG')
plt.plot(timevec,np.mean(resdat,1),label='Residual')

plt.xlabel('Time (ms)')
plt.legend()
plt.show()
```


```python
# show all trials in a map
clim = [-1,1]*20

plt.subplot(131)
plt.imshow(eyedat.T)
plt.title('EOG')


plt.subplot(132)
plt.imshow(EEGdat.T)
plt.title('EOG')


plt.subplot(133)
plt.imshow(resdat.T)
plt.title('Residual')

plt.tight_layout()
plt.show()
```


```python

```
