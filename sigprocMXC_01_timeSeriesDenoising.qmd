---
title: Time series denoising
jupyter: python3
---

## Imports and signal generation

We will create a signal with a sampling rate of 1000Hz (1000 samples/s), and a
length of three seconds (@fig-signal-gen). It will be created by linear interolation between some 
random time points $(p = 15)$.

```{python}
#| label: fig-signal-gen
#| fig-cap: "Python implementation of the mean filter"

import numba as nb
import numpy as np
import matplotlib.pyplot as plt
from pprint import pprint
import scipy.io as sio
import scipy.signal
from scipy.signal import windows as win
import scipy.stats as stats
from scipy import *
import copy

# set the seed
np.random.seed(42)

# create signal
srate = 1000 # Sampling rate (samples/sec, Hz.)
freq = 1/srate  # Frequency
time = np.arange(0, 3, freq) # three seconds total time
n = len(time)  # Sample count
p = 15 # poles for random interpolation

# amplitude modulator and noise level
signal_amp = 30
pure_signal = np.interp(
    x=np.linspace(0, p, n),  # interpolated x coordinate
    xp=np.arange(0, p),  # original x coordinates
    fp=np.random.rand(p) * signal_amp  # y coordinates
    )

# noise level (amplitude), measured in standard deviations
noise_amp = 5
noise  = noise_amp * np.random.randn(n)

# Add noise to the signal
noisy_signal = pure_signal + noise

plt.plot(noisy_signal)
plt.plot(pure_signal)
```

## VIDEO: Mean-smooth a time series

_Mean filter works by setting each data point in the filtered signal to be an
average of the surrounding points from the original signal._ @eq-mean-filter

$$
    y_t = \frac{1}{ 2k + 1 } \sum^{t+k}_{i=t-k} x_i
$$ {#eq-mean-filter}

_Now the number of data points that you go backwards in time and forwards in
time ($k$) is the key parameter of the mean smoothing filter, and that's
called the order of the filter._

_So each time point $t$ in the filtered signal $y$ is defined as the sum of all
the data points in the original signal $x$ going $t$ backwards in time, $k$ points
and $t$ forwards in time $k$ points._

_So here you sum all of these up and then you need to divide by the number of
time points, which is two K plus one._

_Here's where I implement the running mean filter in the time domain.
You will see in the section of this course on convolution that it's also
possible to implement this kind of filter in the frequency domain using
spectral multiplication. That's thanks to something called the convolution
theorem._

```{python}
#| label: fig-mean-filter
#| fig-cap: "Python implementation of the mean filter"

@nb.jit
def mean_filter(signal, radius):

    # initialize filtered signal vector
    n = len(signal)
    filtered = np.full(n, np.nan)

    # implement the running mean filter
    k = radius # filter window is actually (k * 2) + 1
    for i in range(k, n - k):
        # each point is the average of k surrounding points
        filtered[i] = np.mean(signal[i-k:i+k+1])
    
    return filtered

radius = 20
mean_filt = mean_filter(noisy_signal, radius=20)
```

_Now I've specified $k$ here in terms of the number of points, however, time
series data, people typically think about time in terms of milliseconds or
seconds or minutes or whatever is the relevant time scale, but some meaningful
time scale not necessarily points. And that's why I convert from points from
this arbitrary index number here into time in milliseconds._

_So this is the formula. Basically, this is saying the total number of points._

_So the total size of the window times 1000 and then divided by the sampling rate._

_And this will give you the window size in milliseconds._

_Now, in this particular case, it turns out that because I set the sampling rate
to be 1000, the window size is the same thing as the window size in points is
the same thing as the window size in milliseconds. But that's generally not the
case for sampling rates other than 1000._

```{python}
#| label: fig-run-mean
#| fig-cap: "plot the noisy and filtered signals"
# compute window size in milliseconds (ms)
window_diameter = 1000*(2 * radius+1) / srate

plt.plot(time, noisy_signal, lw=.1, label='Original')
plt.plot(time, mean_filt, color='k', label='filtered')

n = len(noisy_signal)
plt.legend()
plt.xlabel('Time (sec.)')
plt.ylabel('Amplitude')
plt.title(f'Running-mean filter with a k={window_diameter:.0f} millisecond window');
```

So there's a couple of interesting things that you can look at here.
First, it's interesting to see the signal still retains some roughness.
If we wanted the signalto be smoother we could increase $k$. At $k = 40$,
looks smoother, but the edges are still present.

Another thing you notice is some funny things happening at the start/end of the
signal called edge effects. If the filter is initialized to zero this will
create awkward jumps at the beginning and at the end.

Another possibility is to initialize the edges to be the original signal.
That doesn't give you this sudden drop down to zero, but it returns a kind of
bizarre looking, filtered signal where there's a lot of really high frequency
activity in the beginning and at the end.

### Closing remarks

So this is the running mean filter. 
It's a very useful filter, particularly when the noise is normally distributed.
This is not an appropriate filter for all kinds of noise.

This is really specific for when noise is distributed, positive and negative
relative to the signal of interest.

### Note: Edge effects

_In general, you always get something bizarre happening at the edges of your
time series Whenever you apply any kind of temporal filter. You will see
these edge effects every time you apply a temporal filter to data, regardless
of the type of filter. So then the question is what do you do with these edge points?_

* _Set them to be the original signal._
* _Ignore them by cropping off the initial/final data points from the filtered signal._

_Unfortunately, there's never a best or optimal way that always works for
dealing with edge effects. So usually you have to figure out what's the best
way to deal with edge effects on a case by case basis given your specific application._


## VIDEO: Gaussian-smooth a time series

Gaussian smoothing is very similar to Mean-smoothing, but it uses a Gaussian
weight instead of a constant weight of $1/n$.

$$
    y_t = \sum^{t+k}_{i=t-k} x_i g_i
$$ {#eq-gauss-filt}

The mean smoothing filter had fraction outside the the sum, dividing the point
count within the kernel ($n = 2k + 1$). The gaussian filter
uses a weighting function $g_i$, that encloses a total area of one.
Its formula typically is defined in terms of the mean ($\mu$) and stadard
deviation ($\sigma$, @eq-gauss-normal) 

$$
    g (x) =
    \frac{1}{\sigma \sqrt{2 \pi}}
    \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2}\right)
    = \frac{1}{\sigma \sqrt{2 \pi}} g_{pt}(t)
$$ {#eq-gauss-normal}

In signal processing the gaussian is defined based on the concept of the
full width at half maximum (@eq-fwhm). FWHM is the horizontal distance between the
sides of the distribution at its the vertical-mid point (50% gain). Ultimately
it reflects the same information as $\sigma$: how wide is the
distribution.

$$
    \mathrm{FWHM} = w = \sigma \sqrt{8 \ln 2}
$$ {#eq-fwhm}

If the kernel is centered at $0$, the formula in terms of FWHM becomes:

$$
    g (x)
    = \frac{2 \sqrt{\ln 2}}{w \sqrt{\pi}}
        \exp \left(  - \frac{4 \ln (2) x^2}{w^2} \right)
    = \frac{2 \sqrt{\ln 2}}{w \sqrt{\pi}} g_{pt} (t)
$$ {#eq-gauss-fwhm}

Gaussian filters can use the the gaussian distribution, or use a gaussian window
($g_{pt}$ in @eq-gauss-normal and @eq-gauss-fwhm).

$$
    \mathrm{window}
    = \exp \left( - \frac{1}{2} \left( \frac{n}{\sigma} \right)^2 \right)
    = \exp \left(  - \frac{4 \ln (2) x^2}{w^2} \right)
$$ {#eq-gauss-win}

A distribution has an area of one, whereas a window has a peak value of 1
(@fig-dist-vs-win red and blue lines, respectively).
This distinction is important because if the kernel is based on the the
distribution no normalization factor is needed. However, if the kernel is based
on the gaussian window it will need to be normalized by dividing by its sum.
Otherwise, the scale of the filtered signal will differ from the original.


```{python}
#| label: fig-dist-vs-win
#| fig-cap: "Distribution vs. Window"
radius_samples = 50
diameter_samples =  2 * radius_samples + 1

mu_x, sigma_x = 0, 1
w_x = sigma_x * np.sqrt(8 * np.log(2))

radius_x = 3 * sigma_x
x = np.linspace(mu_x - radius_x, mu_x + radius_x, diameter_samples)

sigma_samples = 50/(radius_x/sigma_x)
x_samples =  np.arange(-radius_samples, radius_samples + 1) # "offset"

y = {}
part_sigma = 1/(sigma_x * np.sqrt(2 * np.pi))
y['$g(\mu, \sigma)$'] = part_sigma * np.exp(-((x - mu_x)**2)/(2 * sigma_x**2))
y['$g(\mu, \sigma)$ (ref.)'] = stats.norm.pdf(x, mu_x, sigma_x)

part_w = (2 * np.sqrt(np.log(2)))/(w_x * np.sqrt(np.pi))
y['$g(w)$'] = part_w * np.exp(- (4 * np.log(2) * x**2)/(w_x**2))

y['Win. $(\sigma)$'] = np.exp( -(1/2) * (x_samples/sigma_samples)**2 )
y['Win. $(\sigma)$ (ref.)'] = signal.windows.gaussian(diameter_samples, sigma_samples)
y['Win. $(w)$'] = np.exp( -(4 * np.log(2) * x**2) / (w_x**2) )
y[r'Win. $(\mu, \sigma) \rightarrow g(\sigma)$'] = part_w * y['Win. $(w)$']
y[r'Win. $(w) \rightarrow g(w)$'] = part_w * y['Win. $(w)$']

for z, (label, yvalues) in enumerate(y.items(), start=1):
    plt.plot(x, yvalues, label=label, zorder=-z)
plt.legend()
```

### An example kernel

We can illustrate the concept using right part of the formula $(g_{r})$ to
make a kernel with $w = 25$. On layman terms, this means we will apply 25 ms.
of smoothing, as most of the weight of the distribution is concetrated within
the FWHM. It is also necessary to define the radius of the
kernel $(k)$, whose length will bo $2k + 1$ time points. These two parameters
should be set ensuring the edges of the distribution defined by $w$
overlap the edges of the widow defined by $k$.

```{python}
#|label: gauss-window
#|caption: "Gaussian window"
@nb.jit
def make_gaussian(srate, radius, fwhm):
    k = radius
    positions = np.arange(-k, k + 1)
    time = positions/srate
    gain = np.exp( -(4 * np.log(2) * time**2) / fwhm**2 )
    return time, gain
```

@fig-gauss-kernel shows three examples with a varying $k$. The first thing
we notice is that the empirical FWHM depicted on the legend is not exactly
the same. This is due several factors, with sampling rate playing the biggest
role (We do not have an infinite number points).

```{python}
#| label: fig-gauss-kernel
#| fig-cap: "Relationship between $k$ and FWHM"
def plot_gaussian_kernel(fwhm, kstyles):
    arrow_kwargs = {'arrowprops': {'arrowstyle': '<->'}}
    style = {'facecolor': 'w', 'edgecolor': 'k', 'boxstyle': 'round,pad=.2'}

    for k, color in kstyles.items():

        times, gains = make_gaussian(srate/1000, radius=k, fwhm=fwhm)
        plt.axhline(.5, color='k', ls=':', zorder=0)
        plt.plot(times, gains, color='k') # Plot gaussian

        fwhm_0 = np.argmin( (gains[:k]-.5)**2 )
        fwhm_1 = np.argmin( (gains[k:]-.5)**2 ) + k # re-set indexing
        emp_fwhm = times[fwhm_1] - times[fwhm_0]
        plt.plot(
            [times[fwhm_0],times[fwhm_1]], [gains[fwhm_0],gains[fwhm_1]],
            'o-', label=f"k = {k}; Emp. FWHM = {emp_fwhm:.0f}") # Emp. FWHM
        
        start, end, gain0 = times[0], times[-1], gains[0] # Win. len
        plt.fill_between(x=times, y1=gain0, y2=gains, color=color)
        plt.annotate('', xy=(start, gain0), xytext=(end ,gain0), **arrow_kwargs)
        plt.text(0, gain0, f"k={k}", ha='center', va='center', bbox=style)

    plt.title(f"Gaussian kernel with target FWHM = {fwhm:.02f} ms")
    plt.xlabel('Time (ms)')
    plt.ylabel('Gain')
    plt.legend()

plot_gaussian_kernel(fwhm=25, kstyles={37: 'gray', 25: 'lightgray', 12: 'w'})
```

When $k = 12$, the kernel is a narrow window with a short time span. In fact,
it is too short, as the gain does not reach the bottom. The empirical FWHM
is shifted upwards, becoming the lowest point of the kernel, instead of the
vertical mid-point. $k = 25$ still is too low, whereas $k = 37$ is a reasonable
value, reaching down to near-zero vales. Note the kernel should not be too wide,
as the computational burden and the edge effects would increase, with very
little impact on the quality of the result. @fig-wide-gaussian shows a kernel
with $k = 100$ window, where most points will have very little impact, as points
past $\pm 30$ have  near-zero gains. Thus, it is good to have the smallest
possible $k$ that tapers down to $\approx 0$.

```{python}
#| label: fig-wide-gaussian
#| fig-cap: "Wide gaussian"
plot_gaussian_kernel(fwhm=25, kstyles={100: 'w', 25: 'lightgray'})
```

Note @fig-gauss-kernel and @fig-wide-gaussian both use $g_{pt}$ to generate the
kernels, which is not the complete formula of the gaussian. Thus, they are not
normalized $\left( \sum g \neq 1 \right)$. If applied, they would return a
filtered signal with a different scale than the original. To avoid this issue
is necessary to use the complete formula, or to normalize by dividing each gain
by the sum of all of them.

## The actual filtering

```{python}
#| label: fig-gaussian-filter
#| fig-cap: "Result of the gaussian filter"

@nb.jit
def gaussian_filter(signal, kernel):

    # initialize filtered signal vector
    filtered = np.full(signal.shape, np.nan)#signal.copy()
    n = len(filtered)

    # Calculate the kernel radius and the normalizer
    k = int((len(kernel) - 1)/2)
    norm_kernel = kernel/np.sum(kernel)

    # Apply the filter
    for i in range(k+1, n-k):
        # each point is the weighted average of k surrounding points
        filtered[i] = np.sum(signal[i-k:i+k+1] * norm_kernel)
    
    return filtered

_, kernel = make_gaussian(srate, radius=50, fwhm=25)
gauss_filt = gaussian_filter(noisy_signal, kernel)

# plot
plt.plot(time, noisy_signal,'gray', lw=.1, label='Original')
plt.plot(time, mean_filt, 'b', label='Mean-filtered')
plt.plot(time, gauss_filt,'r', label='Gaussian-filtered')

plt.xlabel('Time (s)')
# plt.xlim(1.7, 1.8)
plt.ylabel('amp. (a.u.)')
plt.legend()
plt.title('Gaussian smoothing filter');
```

The result of a gaussian filter tends to be smoother than those of its
mean-based counterpart, albeit the edges are even more roughen up.

The Gaussian filter  gives us a smoother time series compared to the running
mean filter. Which one is more useful is application-dependant. These two
filters are particularly useful when the noise is normally distributed around
the signal.

# VIDEO: Gaussian-smooth a spike time series

I'm going to show you the impact of applyng a Gaussian smoothing filter over
a spiky time series.

## The time series

The time series will contain 300 spikes, whose separation will be defined by
an exponential distribution, which is $e$ to the some random numbers.
What's interesting about these exponential distributions like this is that
the spikes will have a burst-like distribution: in some cases they appear in
quick succession, whereas they can be widely spaced in other cases.

```{python}
#| label: fig-spike-times
#| fig-cap: "Histogram of the wait time between spikes"

## generate time series of random spikes

# number of spikes
spike_count = 300

# inter-spike intervals (exponential distribution for bursts)
spike_spaces = np.round(np.exp( np.random.randn(spike_count) )*10)

# generate spike time series
spiky_signal = np.zeros(int(sum(spike_spaces)))
for i in range(0, spike_count):
    spiky_signal[ int(np.sum(spike_spaces[0:i])) ] = 1

plt.hist(spike_spaces, bins='auto');
```

As you can see in @fig-fwhm-effect, most spikes happen in quick succession, whereas the longer the
interval, the more unlikely it becomes.

Now we will apply the Gaussian smoothing filter we saw earlier. In that context,
we were multiplying by 1000 and dividing by the sampling rate to convert the
time vector to milliseconds.

```{python}
#| label: fig-fwhm-effect
#| fig-cap: "Impact of the FWHM"

# plot the filtered signal (spike probability density)
plt.plot(spiky_signal, 'lightgray', lw=.1, label='spikes')
for fwhm, w in {5: 1, 25: 2, 95: 3}.items():
    _, the_kernel = make_gaussian(1, radius=100, fwhm=fwhm)
    plt.plot(gaussian_filter(spiky_signal, the_kernel), label=str(fwhm), lw=w)

plt.ylim(bottom=0, top=.45)
plt.legend()
plt.title('Spikes and spike probability density');
```

In @fig-fwhm-effect you can interpret the colored lines as the probability of
observing a spike at any given point of time. his red line could have a simpler
interpretation, or be more amenable to certain kinds of analyses.

The impact of the filter is largely dependant on this parametrization.
Fore example, a window with $FWHM = 5$ barely performs some smoothing between
spikes. The result with $FWHM = 25$ is reasonable, whereas $FWHM = 95$
has a heavily smoothed result. Ultimately, the parametrization will depend on
the specific application.

In general it is a good practise to find some extreme bounds outside of which
the parametrization does not really make sense, and work from there to find
appropiate values for the specific dataset. Going back to our  example, 15
could be a reasonable value, showing rasonable time precision and variability.

# VIDEO: Denoising via TKEO

_EMG stands for Electromyogram. The Myo refers to muscle and the gram is for
measurement, and the electrode, of course, is for electricity.
When you want to move your body somehow or grasp an object that's done through
muscles, of course, and those muscles are moving because something in your
brain is telling them to move. So there is an electrical signal that is
generated in the brain. It travels down the spinal cord and that activates the
muscles, for example, in your hand._

_Now, if you were to measure from two electrodes that are closely spaced to
each other like this, you would get an electrical (EMG) signal as shown in 
@fig-emg. The EMG is used for many purposes, one of which is to determine when a
movement was initiated._

![EMG](sigprocMXC_01_timeSeriesDenoising/emg.png){#fig-emg}

_

@fig-tkeo shows an EMG signal. _You can see it's a little bit noisy, there's
some slow drifts, there's some fast activity_ and a couple of muscle twitches,
ones larger than the others.

_It's often of interest or clinical or scientific importance to determine
exactly when a muscle was initiated. When you're doing this visually, it's
often pretty easy to do because you just kind of look at it and you can spot
when the onset happens. But for computer algorithms, it can sometimes be a
little difficult to distinguish the background noise from the EMG signal of
interest. Note this signal is quite clean!, sometimes the noise is as large as
big as the mid-sized spikes we see._

_There is a general denoising strategy abbreviated as TKEO, the 
Kaiser Energy Tracking Operator. Each time point $t$ in the filtered signal
$y$ is:_

$$
    y_t = x_t^2 - x_{t-1}x_{t+1}
$$

The impact of the filter is suppressing the noise while enhancing the signal.
Let's load the data in Python and take a look on its contents.

```{python}
#| label: fig-emgdata
#| caption: "EMG"
# import data
emgdata = sio.loadmat('sigprocMXC_01_timeSeriesDenoising/emg4TKEO.mat')

emg_time = emgdata['emgtime'][0]
emg_measured = emgdata['emg'][0]
pprint(emgdata)
```

_There are three variables f fs that's probably the frequency of sampling,
EMG time and the signal. If we plot the last two, the $x$ axis whould be the
time in milliseconds, and $y$ would be the energy measurements measurements in 
microvolts (e.g., 100 microvolts)._

```{python}
## plot
# plot "raw" (normalized to max.1)
plt.plot(emg_time, emg_measured,'b',label='EMG')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude or energy')
plt.legend()
```

_Here the filtered version of the signal is the original signal, and then we
loop between the second and the second to-last point (the extremes are needed
for filtering). Thus, these two points will be affected by edge effects._

So each time point I in the filtered signal is the $i^{th}$ time point in the
data squared minus the product of the previous time point and the following
time point.

The algorithm can be applied within a for loop, or using vectorization. The
former is easier to interpret, whereas the latter is advisable to retain 
an adequate performance.


```{python}
#| label: fig-tkeo
#| caption: "EMG"#| 

looped = copy.deepcopy(emg_measured)
for i in range(1, len(looped)-1):
    looped[i] = emg_measured[i]**2 - emg_measured[i-1] * emg_measured[i+1]

vectorized = copy.deepcopy(emg_measured)
vectorized[1:-1] = emg_measured[1:-1]**2 - emg_measured[0:-2] * emg_measured[2:]
```

Note the original EMG signal and the filtered EMG signal, are no longer in 
the same scale: the original is in Microvolts , whereas the filtered signal 
now is in microvolt squared. A possibility to visualize them together is to 
normalize the signals according to their maximum to plot them
on the same graph. The filtered signal becomes a near-flat line.

```{python}
## plot
# plot "raw" (normalized to max.1)
plt.plot(emg_time, emg_measured/np.max(emg_measured),'b',label='EMG')
plt.plot(emg_time, looped/np.max(looped),'m',label='TKEO energy')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude or energy')
plt.legend()
```

To make this comparison it is useful to convert
them to z-score, which is is defined as the signal minus its mean 
divided by the standard deviation of the signal. In our example both statistics
are calculated with the baseline of the data before 0 instead of all data
points.This ensures variations appearing later on will be heavily enhanced.

```{python}
## convert both signals to zscore
def zscore(time, signal):
    idx0 = np.argmin(time**2)
    baseline = signal[0:idx0]
    return (signal - np.mean(baseline))/np.std(baseline)

# plot zscored
plt.plot(emg_time, zscore(emg_time, vectorized),'m',label='TKEO energy')
plt.plot(emg_time, zscore(emg_time, emg_measured),'b',label='EMG')
plt.xlabel('Time (ms)')
plt.ylabel('Zscore relative to pre-stimulus')
plt.legend();
```

Z-scores really show the difference in the signal-to-noise ratio between the
original and the TKEO-filtered signal. The latter has score reaching of 800, 
800 standard deviations greater than the pre-stimulus mean of the baseline
period, whereas the original signal only reaches 20 for the largest event.
All peaks are easier to detect when the EMG signal has been converted into
energy compared to the original signal.

# VIDEO: Median filter to remove spike noise

The median filter is great for removing large spikes in time series data.
It is calculated by sorting the data points by magnitude, and taking the 
central value. If the number of observations is even the median is the
mid-point between the observations closest to the centre. By picking this
point, it avoids the influence of outliers.

Is important to note median is not a linear (liner time complexity?). Thus,
it is best to apply the median it over known outliers (or missing values),
not over all points.

In this case we will generate a signal that is the integral of random numbers
(integrated white noise, Brownian noise). Some values (5%) will be replaced
with abnormally large values.

```{python}
# create signal
n = 2000
signal_z = np.cumsum(np.random.randn(n))


# proportion of time points to replace with noise
propnoise = .05

# find noise points
noisepnts = np.random.permutation(n)
noisepnts = noisepnts[0:int(n*propnoise)]

# generate signal and replace points with noise
signal_z[noisepnts] = 50 + np.random.rand(len(noisepnts)) * 100


# use hist to pick threshold
plt.plot(signal_z);
#plt.hist(signal,100);
```

Ultimately, there are "single" contaminate data points. Applying a mean or a
gaussian filter would not be very useful, as these would be swayed by the large 
values. In cases like this, we just want to remove the spikes whilst
retaining the rest of the points, something we can do with the median filter.

Before applying the filter itself, we need to identify the spikes. For example,
values above a threshold based on preexisting knowledge. One way to set that
threshold would be to calculate the histogram as a way to identify outliers.
For example, we could set the threshold at 57. 

```{python}
# use hist to pick threshold
plt.hist(signal_z,100);
```

Now we identify the data points exceedding the aforementioned threshold.
Then, we initialize the filtered signal to be the original signal.
Then we loop along the data points where the threshold has been exceeded, and
substitute the abnormal value by the median of the neighboring points within a
radius $k$, i.e., from $t-k$ to $t+k$. Note spikes could occur near the
start/end of the signal.

To avoid this issue is advisable to use flexible boundary algorithm: the lower
bound is going to be this time point minus k or one [zero for python],
whichever is larger, and the last point must be the last data point or smaller.

If we plot the signal we can see the signal is not changed except for these
large spike points. And this is really what the median filter is optimal for.

If the median filter is applied along the time series itself (not just a
selection), we see a lot of dynamics have been lost, which may not be 
acceptable.

Also note the threshold needs to be picked quite carefully, or perhaps it
should be should be a local threshold.... but that's a topic for a different
video.

```{python}
# visual-picked threshold
threshold = 40

# find data values above the threshold
suprathresh = np.where( signal_z > threshold )[0]

# initialize filtered signal
filtsig = copy.deepcopy(signal_z)

# loop through suprathreshold points and set to median of k
k = 20 # actual window is k*2+1
for ti in range(len(suprathresh)):
    
    # lower and upper bounds
    lowbnd = np.max((0,suprathresh[ti]-k))
    uppbnd = np.min((suprathresh[ti]+k+1,n))
    
    # compute median of surrounding points
    filtsig[suprathresh[ti]] = np.median(signal_z[lowbnd:uppbnd])

# plot
plt.plot(range(0,n),signal_z, range(0,n),filtsig);

```


---
# VIDEO: Remove linear trend
---

Sometimes there is a trend, a slow drift in time series data. This may be a
real tendency, or a an artifact of the system. Nonetheless, if we are not
interested in said trend, we can just apply linear detrending.

Linear detrending fits a single straight line through the entire dataset, and
then subtract the line values from each of the original points, retaining
just the local fluctuations.

The signal is created as the cumulative sum of some random noise (brownian
noise) plus a linear trend ranging from $-30$ to $+30$. The latter is removed
using the detrend function, and then both the detrended and the original signal
are plotted. Please note after detrending te mean of the signal becomes
$\approx 0$ (basically zero plus computer rounding error).

If we compare both the original and the detrended signals we see local
fluctuations have been retained, whereas the trend has been removed.

```{python}
# create signal with linear trend imposed
n = 2000
trended_signal = np.cumsum(np.random.randn(n)) + np.linspace(-30, 30, n)

# linear detrending
detsignal = scipy.signal.detrend(trended_signal)

# get means
omean = np.mean(trended_signal) # original mean
dmean = np.mean(detsignal) # detrended mean

# plot signal and detrended signal
plt.plot(trended_signal,label=f"Original, mean={omean:.2f}")
plt.plot(detsignal,label=f"Detrended, mean={dmean:.2f}")
plt.legend();
```


---
# VIDEO: Remove nonlinear trend with polynomials
---

Not all trends that may appear in a dataset are linear. The solution in this
case would be to use a polynomial, a function with form

\begin{equation}
    \beta_0 + \beta_1 x + \beta_2x^2 + \dots + \beta_n x^n
\end{equation}


It's a function of some variable $x$ (e.g., time) accompanied by several 
coefficients $\left( \beta_0, \beta_1, \beta_2, \dots, \beta_n \right)$,
each multiplying accompanying terms with increasingly higher powers
$\left( x^{0}, x^{1}, x^{2}, \dots, x^{n} \right)$ ($\beta_0$ is a general
offset tat would be accompanied by $x^{0}$).

## Polynomials intuition

Now we will create a polynomial. $x$ ranges between $-15$ and $+15$ in 100
steps. The signal $y$ is $x^{n}$ up to max_order multipled by a random number
from the gaussian distribution.

```{python}
#| label: fig-polynomial-line
#| fig-cap: "Thirt dorder polynomial"
max_order = 3
sample_count = 100
x = np.linspace(-15, 15, sample_count)

y = np.zeros(sample_count)
for order in range(max_order + 1):
    y += np.random.randn() * (x**order)

plt.plot(y)

```

With order $0$ the polynomial is just a flat line with some offset, with
order $1$ is a sloped line, with order $2$ is a parabola, and with order $3$
it becomes as shown in @fig-polynomial-line.

## Create a signal with a slow drift

Now we will create a signal by interpolating between a bunch of random numbers:

```{python}
## generate signal with slow polynomial artifact

n = 10000
t = range(n)
k = 10 # number of poles for random amplitudes

slowdrift = np.interp(
    np.linspace(1,k,n), # x-coord. where the interpolated values are evaluated
    np.arange(0,k), # original x-coordinates
    100 * np.random.randn(k) # original y-coordinates
    )
signal = slowdrift + 20 * np.random.randn(n)

# plot
plt.plot(t,signal)
plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude');
```

We will use polyfit to remove the the slow drift (third order). The function
returns a vector/array with the same number of coefficients as the order plus
one for the offset $\left( \beta_0, \beta_1, \beta_2, \beta_3 \right)$. Then,
we get the predicted values from polyval, and obtain the residual.
In a ideal case (the fit perfectly captures the data), the residual would be
$0$. As you can see in @fig-naive-polyfit, the result is not very good, as the
fit line (red) does not really capture the drift.

```{python}
#| label: fig-naive-polyfit
#| fig-cap: "Fit a 3-order polynomial"

# polynomial fit (returns coefficients)
coefficients = np.polyfit(t, signal, 3)
print(coefficients)

# predicted data is evaluation of polynomial
predicted = np.polyval(coefficients, t)

# compute residual (the cleaned signal)
residual = signal - predicted

# now plot the fit (the function that will be removed)
plt.plot(t,signal, 'b',label='Original')
plt.plot(t,predicted, 'r',label='Polyfit')
plt.plot(t,residual, 'k',label='Filtered signal')
plt.legend();
```

Ultimately, the failure to capture the data stems from an inadequate polynomial
order. Said optimum can be estimated using Bayes information criteria (BIC).
What the BIC does is to asses how well the your model fits a particular data
set.

\begin{equation}
    \epsilon = n^{-1} \sum^{n}_{i=1} \left(\hat{y}_i - y_{i} \right)^{2}
\end{equation}

\begin{equation}
    b = n \ln (\epsilon) + k \ln (n)
\end{equation}

$\epsilon$ is the mean squared error, which is calculated from the sample
count, and the difference between the actual data ($y$) and the model
prediction ($\hat{y}$), the value given by the polynomial (red line in
@fig-naive-polyfit). $k$ is used to account for the number of free parameters in
the model. Using more parameters may mean more flexibility to fit the data, but
also means the model becomes more more complex. That's why $b$ includes this
offset, to acount the tradeoff between accuracy and complexity. On coarse terms,
the fit is better the samaller the number of aparameters and the smaller the
difference between data and model (it can reach negative values if
$\epsilon < 1$).

The order with the smallest BIC gives you the model with the best accuracy /
complexity when applied to the data. 

In figure FIGURE we loop from orders ranging from orders 5 to 40, make a
prediction, calculate MSE and calculate the BIC. Note python will spit a bunch
of warnings indicating the fits are terrible. Note even tho


```{python}
## Bayes information criterion to find optimal order

# possible orders
orders = np.arange(5,40)

# sum of squared errors (sse is reserved!)
sse1 = np.zeros(len(orders))

# loop through orders
for ri in range(len(orders)):
    
    # compute polynomial (fitting time series)
    yHat = np.polyval(np.polyfit(t,signal,orders[ri]),t)
    
    # compute fit of model to data (sum of squared errors)
    sse1[ri] = np.sum( (yHat-signal)**2 )/n


# Bayes information criterion
bic = n*np.log(sse1) + orders*np.log(n)

# best parameter has lowest BIC
bestP = min(bic)
idx = np.argmin(bic)

# plot the BIC
plt.plot(orders,bic,'ks-')
plt.plot(orders[idx],bestP,'ro')
plt.xlabel('Polynomial order')
plt.ylabel('Bayes information criterion');
```

```{python}
is_large = (orders > 25)
plt.plot(orders[is_large], bic[is_large], 'ks-')
plt.xlabel('Polynomial order')
plt.ylabel('Bayes information criterion');
```

```{python}
## now repeat filter for best (smallest) BIC

# polynomial fit
polycoefs = np.polyfit(t,signal,orders[idx])

# estimated data based on the coefficients
yHat = np.polyval(polycoefs,t)

# filtered signal is residual
filtsig = signal - yHat

## plotting
plt.plot(t,signal,'b',label='Original')
plt.plot(t,yHat,'r',label='Polynomial fit')
plt.plot(t,filtsig,'k',label='Filtered')

plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude')
plt.legend();
```

After certain point, it seems that all of subsequent values are equally low.
However, this is skewed by the high values appearing for small orders.
Note a clear minimum may not appear, indicating increasingly complex
polynomials seem fit the data equally well. In cases like that is better to use
the polynomial with the smallest order (Occam's razor).

# Averaging multiple repetitions (time-synchronous averaging)

One very simple and yet very effective way to increase the signal to noise
ratio of a signal is by averaging together multiple repetitions of some event
in that signal (time synchronous averaging).

Here thie signal is a 1d long time series, and we cut it into little epochs
or snippets aligned to the onset of some event. By averaging all of these
**windows** together, we can attenuate the noise while preserving the signal,
obtaining a much cleaner version of the signal.

## Event genration

The event we will use is the derivative of a Gaussian. Then, we are going to
insert 30 of these events into random positions within a data set. Note the
insertion of the events also considers its length ($k$). Finally, we add noise
about the same magnitude as the signal, and plot. You may notice the result
may not really resemble the original event.

```{python}
## simulate data

# create event (derivative of Gaussian)
k = 100 # duration of event in time points
event = np.diff(np.exp( -np.linspace(-2,2,k+1)**2 ))
event = event/np.max(event) # normalize to max=1

# event onset times
Nevents = 30
onsettimes = np.random.permutation(10000-k)
onsettimes = onsettimes[0:Nevents]

# put event into data
data = np.zeros(10000)
for ei in range(Nevents):
    data[onsettimes[ei]:onsettimes[ei]+k] = event

# add noise
data = data + .5*np.random.randn(len(data))

# plot data
plt.subplot(211)
plt.plot(data)

# plot one event
plt.subplot(212)
plt.plot(range(k), data[onsettimes[3]:onsettimes[3]+k])
plt.plot(range(k), event);
```



```{python}
## extract all events into a matrix

datamatrix = np.zeros((Nevents,k))

for ei in range(Nevents):
    datamatrix[ei,:] = data[onsettimes[ei]:onsettimes[ei]+k]

plt.imshow(datamatrix)
plt.xlabel('Time')
plt.ylabel('Event number')
plt.title('All events')
plt.show()

plt.plot(range(0,k),np.mean(datamatrix,axis=0),label='Averaged')
plt.plot(range(0,k),event,label='Ground-truth')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.legend()
plt.title('Average events')
plt.show()
```

If we do know the actual onsets of these events occurred, we can loop through
all of the different events in the dataset and generate a data matrix where the
rows represent the individual events whereas the columns represent time.

In figure FIGURE the first panel represents the amplitude of the signal 
converted into color, where each row represents one of the 30 events. The
second represents the 'average' event. There  is some noise remaining, but
overall it is cleaner than the the individual events. Note this procedure only
will work if the onset of the events is known, making necessary some sort of
template/pattern matching algorithm to identify the events.


# Remove artifact via least-squares template-matching

Imagine you have some signal changing over time (data channel), and a second
ancillary measurement showing whether there might be an artifact. 
Then, we can look for a matching pattern between both and use it to remove
the artifact from the data channel. The procedure is called least squares
fitting, and is pretty much the same as ordinary/general linear models.

The algorithm is applied by creating a regression model based on least squares,
which returns the parameter vector $\beta$ (regression weights), the closest
match between artifact (predictor, column vector $X$), and the data channel
(predicted, vector $y$, the time series as such).

\begin{equation}
    \beta = \left( X^T X \right)^{-1}X^{T}y
\end{equation}

Once the best match has been found, you scale the *design matrix* ($X$) with
\beta ($X\beta$, predicted data), and subtract the result form the observed data.

Note the predicted data is the component in the data vector that is best
explained by the design matrix containing the artifact signal.

\begin{equation}
    r = y - X \beta
\end{equation}

the residual ($r$) is a vector of the same size as the original signal $y$, but
with the algorithm removed based on the predicted data. Tis residual would be
fed into the subsequent analyses with its artifacct subtracted out.

If you have a single channel that provides the artifact, the design matrix will
have two columns. The first would be just ones for the independant variable
time, and $a_{1} \dots a_{n}$ would be the artifact channel

\begin{bmatrix}
    1 & a_{1} \\
    1 & a_{2} \\
    1 & a_{3} \\
    \vdots & \vdots \\
    1 & a_{n} \\
\end{bmatrix}

These equations boil down to calculating a regression model, predicting the
data channel from the artifact channel, and use said model to remove the
artifact from the data. All these details are just linear algebra.

```{python}
# load dataset
matdat = sio.loadmat('sigprocMXC_01_timeSeriesDenoising/templateProjection.mat')
print(matdat)
EEGdat = matdat['EEGdat']
eyedat = matdat['eyedat']
timevec = matdat['timevec'][0]
MN = np.shape(EEGdat) # matrix sizes

plt.plot(EEGdat[:, 0], label='Brain');
plt.plot(eyedat[:, 0], label='Eye');
plt.legend()
```

matdat contains some 707 tests with 15000 data points each.
encephalography data (EEG), electrical measurements of
brain activity. The movement of the eyes can cause artifacts that are present
in the EEG channels that are placed on the head. Thus, it is important to
separate the artifact coming from the eyeball, and our data of interest coming
from the brain.

We want to map the artifact channel onto the data channel and remove whatever
is common between these two channels. We initalize the residual data, and then
we will loop over trials, the 707 repetitions

```{python}
# initialize residual data
resdat = np.zeros(np.shape(EEGdat))


# loop over trials (columns)
for triali in range(MN[1]):
    
    # build the least-squares model as intercept and EOG from this trial
    # column of ones and column of artifact time series
    X = np.column_stack((np.ones(MN[0]),eyedat[:,triali])) # design matrix
    
    # compute regression coefficients for EEG channel
    b = np.linalg.solve(X.T@X,X.T@EEGdat[:,triali])
    # Is adisable to avoid explicitly inverting a matrix.
    # Matlab has a slash operator for this purpose
    
    # predicted data
    yHat = X@b
    
    # new data are the residuals after projecting out the best EKG fit
    resdat[:,triali] = EEGdat[:,triali] - yHat

### plotting

# trial averages
plt.plot(timevec,np.mean(eyedat,axis=1),label='EOG')
plt.plot(timevec,np.mean(EEGdat,axis=1),label='EEG')
plt.plot(timevec,np.mean(resdat,1),label='Residual')

plt.xlabel('Time (ms)')
plt.legend()
plt.show()
```

The figure FIGURE shows the average across all the repetitions for the origial
data (orange), the electro-optical ground (EOG, blue, the artifact), and the
residual (green): the EEG without the impact of eye movement.

```{python}
# show all trials in a map
clim = [-1,1]*20

fig, axes = plt.subplots(ncols=3, sharex=True, sharey=True)
pairs = {'EOG': eyedat, 'EEG': EEGdat, 'Residual': resdat}
for ax_id, (title, data) in enumerate(pairs.items()):
    axes[ax_id].set_title(title)
    axes[ax_id].imshow(data.T)

fig.tight_layout()
```

The $y$ axis in figure FIGURE is thrials, and $x$ is the trial. The color
depicts the amplitude of the signal, ranging from darker (low values) to
brighter colors (high values)

The first panel depcts the data from the eye, the second the EEG, and the
third removed the former from the latter, enabling exploration of smaller
features. Note the color scale is the same across plots, indicating even though
amplitude has decreased, there are some remaining prominent features in the
residual.

# Exercise

```{python}
# Read the files, put them into easy to use variables
data = sio.loadmat('sigprocMXC_01_timeSeriesDenoising/challenge.mat')
data = {k: v for k, v in data.items() if not k.startswith('_')}
original, target = data['origSignal'][0], data['cleanedSignal'][0]
```

```{python}
# Plot the data distribution
plt.hist(original, bins='auto');

# Calculate median-based statistics
median = np.median(original)
mad = np.median(np.abs(original - median))
sd = (mad/0.67449)

# Plot a prospective cutoff point
cut = 3.5
plt.axvline(median - cut * sd, color='k')
plt.axvline(median + cut * sd, color='k');
```

```{python}
# Replace outliers with the median of data within a radius

# Identify
outlier_idx = np.argwhere(np.abs(original - median) > cut * sd)

# Substitute
sel_radius = 3
despiked = original.copy()
for idx in outlier_idx.flatten():
    start = (idx - sel_radius)
    end = (idx + sel_radius + 1)
    despiked[idx] = np.median(original[start:end])

plt.plot(original, label='original')
plt.plot(despiked, label='despiked')
plt.legend();
```

```{python}
# Prepare the kernels
conv_radius = 75
conv_diameter = 1 + 2 * conv_radius
the_kernels = {
    'mean': np.ones(conv_diameter),
    'gauss': win.gaussian(conv_diameter, std=conv_diameter/3)
}

# Plot the results of the filtering
plt.plot(despiked, label='despiked', lw=.25, alpha=.5)
for label, kernel in the_kernels.items():
    kernel /= np.sum(kernel) 
    plt.plot(np.convolve(despiked, kernel, mode='same'), label=label)
plt.legend();
plt.ylim(-1.5, 1.5)
```