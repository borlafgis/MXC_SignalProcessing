---
title: Quarto Computations
jupyter: python3
---

## Imports and signal generation

We will create a signal with a sampling rate of 1000Hz (1000 samples/s), and a
length of three seconds. It will be created by linear interolation between some 
random time points ($p = 15$).

```{python}
import numba as nb
import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import scipy.signal
from scipy import *
import copy

# set the seed
np.random.seed(42)

# create signal
srate = 1000 # Sampling rate (samples/sec, Hz.)
freq = 1/srate  # Frequency
time = np.arange(0, 3, freq) # three seconds total time
n = len(time)  # Sample count
p = 15 # poles for random interpolation

# amplitude modulator and noise level
signal_amp = 30
pure_signal = np.interp(
    x=np.linspace(0, p, n),  # interpolated x coordinate
    xp=np.arange(0, p),  # original x coordinates
    fp=np.random.rand(p) * signal_amp  # y coordinates
    )

# noise level (amplitude), measured in standard deviations
noise_amp = 5
noise  = noise_amp * np.random.randn(n)

# Add noise to the signal
noisy_signal = pure_signal + noise

plt.plot(noisy_signal)
plt.plot(pure_signal)
```

## VIDEO: Mean-smooth a time series

_Mean filter works by setting each data point in the filtered signal to be an
average of the surrounding points from the original signal._

\begin{equation}
    y_t = \frac{1}{ 2k + 1 } \sum^{t+k}_{i=t-k} x_i
\end{equation}

_Now the number of data points that you go backwards in time and forwards in
time ($k$) is the key parameter of the mean smoothing filter, and that's
called the order of the filter._

_So each time point $t$ in the filtered signal $y$ is defined as the sum of all
the data points in the original signal $x$ going $t$ backwards in time, $k$ points
and $t$ forwards in time $k$ points._

_So here you sum all of these up and then you need to divide by the number of
time points, which is two K plus one._

_Here's where I implement the running mean filter in the time domain.
You will see in the section of this course on convolution that it's also
possible to implement this kind of filter in the frequency domain using
spectral multiplication. That's thanks to something called the convolution
theorem._

```{python}

@nb.jit
def mean_filter(signal, radius):

    # initialize filtered signal vector
    n = len(signal)
    filtered = np.full(n, np.nan)

    # implement the running mean filter
    k = radius # filter window is actually (k * 2) + 1
    for i in range(k, n - k):
        # each point is the average of k surrounding points
        filtered[i] = np.mean(signal[i-k:i+k+1])
    
    return filtered

radius = 20
mean_filt = mean_filter(noisy_signal, radius=20)
```

_Now I've specified $k$ here in terms of the number of points, however, time
series data, people typically think about time in terms of milliseconds or
seconds or minutes or whatever is the relevant time scale, but some meaningful
time scale not necessarily points. And that's why I convert from points from
this arbitrary index number here into time in milliseconds._

_So this is the formula. Basically, this is saying the total number of points._

_So the total size of the window times 1000 and then divided by the sampling rate._

_And this will give you the window size in milliseconds._

_Now, in this particular case, it turns out that because I set the sampling rate
to be 1000, the window size is the same thing as the window size in points is
the same thing as the window size in milliseconds. But that's generally not the
case for sampling rates other than 1000._

```{python}
# compute window size in milliseconds (ms)
window_diameter = 1000*(2 * radius+1) / srate

#| label: fig-run-mean
#| fig-cap: "plot the noisy and filtered signals"
plt.plot(time, noisy_signal, lw=.1, label='Original')
plt.plot(time, mean_filt, color='k', label='filtered')

n = len(noisy_signal)
plt.legend()
plt.xlabel('Time (sec.)')
plt.ylabel('Amplitude')
plt.title(f'Running-mean filter with a k={window_diameter:.0f} millisecond window')

plt.show()

```

So there's a couple of interesting things that you can look at here.
First, it's interesting to see the signal still retains some roughness.
If we wanted the signalto be smoother we could increase $k$. At $k = 40$,
looks smoother, but the edges are still present.

Another thing you notice is some funny things happening at the start/end of the
signal called edge effects. If the filter is initialized to zero this will
create awkward jumps at the beginning and at the end.

Another possibility is to initialize the edges to be the original signal.
That doesn't give you this sudden drop down to zero, but it returns a kind of
bizarre looking, filtered signal where there's a lot of really high frequency
activity in the beginning and at the end.

### Closing remarks

So this is the running mean filter. 
It's a very useful filter, particularly when the noise is normally distributed.
This is not an appropriate filter for all kinds of noise.

This is really specific for when noise is distributed, positive and negative
relative to the signal of interest.

### Note: Edge effects

_In general, you always get something bizarre happening at the edges of your
time series Whenever you apply any kind of temporal filter. You will see
these edge effects every time you apply a temporal filter to data, regardless
of the type of filter. So then the question is what do you do with these edge points?_

* _Set them to be the original signal._
* _Ignore them by cropping off the initial/final data points from the filtered signal._

_Unfortunately, there's never a best or optimal way that always works for
dealing with edge effects. So usually you have to figure out what's the best
way to deal with edge effects on a case by case basis given your specific application._


## VIDEO: Gaussian-smooth a time series

Gaussian smoothing is very similar to Mean-smoothing, but it uses a Gaussian
weight instead of a constant weight of $1/n$. The formula looks like this:

\begin{equation}
    y_t = \sum^{t+k}_{i=t-k} x_i g_i
\end{equation}

The formula is nearly the same as in the case of the gaussian smoothing filter.
The mean smoothing filter had another term outside the summation here, and that
term was dividing by the total number of points in the kernel, which was
$2k + 1$. The gaussian filter uses a weighting function $g_i$, a function that
encloses an area of one (total probability density is one).

The result of a gaussian filter tends to be smoother than those of its
mean-based counterpart, albeit the edges are even more roughen up.

### The gaussian window/kernel

A Gaussian  starts at zero and it ends at zero and it goes up to one. The
formula is:

\begin{equation}
    g = \exp \left( \frac{-4 \ln (2) t^2}{w^2} \right)
\end{equation}

The important thing about this formula to recognize is $e$ ($\exp$) to the
$-t^2$ over something. The $t$ refers to time and it's good to have time be
centered at zero because the maximum weight is reached (one?) at that point,
$t=0$

Note this is a more intuitive formulation of the Gaussian that allows you to
specify $w$ parameter, which controls the width of the distribution in terms
of a quantity called full width at half maximum (FWHM).

FWHM is the distance horizontal between the two closest points to the
vertical-mid point of the distribution (50% gain). It is an intuitive parameter
because you can think about it in terms of how much smoothing you want to apply.

So if you want to apply, let's say ten milliseconds of smoothing, you set
$w = 10$.

#### Python implementation.

For our initial example the full width at half maximum is 25 ($FWHM = 25$).
The time vector used to create the Gaussian, goes from $-k$ to $+k$ (plus one 
to account for Python's zero-based indexing). $k$ is set to $100$.

```{python}


## create Gaussian kernel
# kernel radius
k = 100

# full-width half-maximum: the key Gaussian parameter
actual_fwhm = 25 # in ms

# normalized time vector in ms

def make_gauss(srate, radius, fwhm):
    k = radius
    positions = np.arange(-k, k + 1)
    time = 1000 * positions/srate
    gain = np.exp( -(4 * np.log(2) * time**2) / fwhm**2 )
    return time, gain


# create Gaussian window
gauss_time, gauss_win = make_gauss(srate, radius=k, fwhm=actual_fwhm)
```

Note the empirical FWHM becomes slightly different when it goes through the
gaussian formula due some factors, with the sampling rate playing the biggest
role.

So it's possible that you can specify something that feels very precise here, but it's not actually going to be implemented as this exact number just because we are sampling.

We don't have an infinite number of time points here.

```{python}
# compute empirical FWHM
after_half = k + np.argmin( (gauss_win[k:]-.5)**2 )
before_half = np.argmin( (gauss_win-.5)**2 )
emp_fwhm = gauss_time[after_half] - gauss_time[before_half]
print(f"{emp_fwhm:.02f}")
```

In fact, I'll even leave this number like this.

And now then I do some plotting here.

So here's what the Gaussian looks like for these parameters that I've set up.

You can see I specified this ludicrously precise, full width at half maximum, and the full width at

half maximum that was empirically obtained was 26 milliseconds.

So it's quite close, but not exact.


```{python}
## create Gaussian kernel
# full-width half-maximum: the key Gaussian parameter
actual_fwhm = 60 # in ms

# normalized time vector in ms
k = 100
gtime = 1000 * np.arange(-k, k+1)/srate

# create Gaussian window
gauss_win = np.exp( -(4 * np.log(2) * gtime**2) / actual_fwhm**2 )

# compute empirical FWHM
after_half = k + np.argmin( (gauss_win[k:]-.5)**2 )
before_half = np.argmin( (gauss_win-.5)**2 )

emp_fwhm = gtime[after_half] - gtime[before_half]

# show the Gaussian
plt.plot(gtime, gauss_win,'k-')
plt.plot(
    [gtime[before_half],gtime[after_half]],  # The x
    [gauss_win[before_half],gauss_win[after_half]],  # The y
    'mo-'  # The color and linestyle
    )

# then normalize Gaussian to unit energy
gauss_win = gauss_win / np.sum(gauss_win)
# title([ 'Gaussian kernel with requeted FWHM ' num2str(actual_fwhm) ' ms (' num2str(emp_fwhm) ' ms achieved)' ])
plt.xlabel('Time (ms)')
plt.ylabel('Gain')

plt.show()
```

Now I want to talk a little bit about this K parameter.

It kind of doesn't really matter how much you set this K to.

There's two considerations.

One is that K should be sufficiently long that the Gaussian goes down to zero on both sides.

Technically speaking, the Gaussian never really gets to exact zero, but it gets very, very, very

close.

So these numbers are really, really tiny down here.

So imagine, for example, if I had set K to be ten, then this is much too small, this is too narrow

for the Gaussian or I should say too little time.

You can see that the gain function never even gets below around two thirds, 0.66 and the full width

at half maximum is also not accurate because the way that this full width at half maximum code is working

is looking for the values that are closest to 0.5, closest to 50% gain, which is what it's supposed

to be doing.

But I set this k parameter to be too small so it never even gets down to 0.5.

So this is not good.

Let's try 50.

50 seems good.

We really get to 50% gain, and this drops down to zero.

Maybe it could even be 40.

It's always good to check these filters that you are applying to your data set.

I will talk much more about how to visualize and qualitatively and quantitatively interpret filter kernels

in the section of this course on filtering.

But for now, suffice it to say that it's always good just to look at anything you are going to be applying

to your time series data to make sure that it looks reasonable.

Okay, so I said there were two considerations for the bounds of K, the lower consideration.

The lower bound is about making sure that the Gaussian goes down to zero.

The upper bound is about making sure that this isn't too long, it doesn't go too far out from zero.

And the reason for that is because of edge effects.

And I will explain a little bit more when I show you the results of the filtering later on.

Okay.

There's one final thing I want to say about this Gaussian, which is that after I do this plotting,

after I plot this function, you can see here, it goes up to one.

Then I change the Gaussian window to be normalized to unit energy.

So I divide the Gaussian window by the sum of all the points in the Gaussian.

That's important for getting the filtered signal to be in the same units at the same scale as the original

signal.

Again, I will come back to this in a few minutes.

Okay.

Now I want to show you quickly implementing this filter.

This is not super interesting because the code is basically exactly the same as you saw in the previous

video on mean smoothing.

So we loop over time points and you set each time point in the filtered signal to be the signal from

i minus K to I plus K times the Gaussian window.

And then we sum over all of those points and then I'm going to plot the original signal and the Gaussian.

That looks something like this.

And now I quickly want to go back and show you the mean smoothing filter for comparison.

So this is exactly the mean smoothing filter that I showed in the previous video.

So at this scale, they look overall the same.

And when I zoom in, you will see some differences.

So the Gaussian filter is quite a bit smoother.

It gives us a smoother time series compared to the running mean filter.

Now, I'm not going to say that one is better than the other.

It's very application specific.

There could be times when having a Gaussian filter is better and there could be times when having a

running mean filter is better.

It's important just to be aware of the different options.

All right.

So that's what it looks like.

I think overall, both these filters are quite nice.

They're quite useful when the noise is normally distributed around the signal.

So when the noise has positive values and negative values.

Okay.

So you notice these edge effects here with the Gaussian filter, and that's really because we had to

start at Element K plus one.

So if I set K to be higher, let's say K is 400.

Now, in this case, you know, the Gaussian is kind of the same here.

So maybe you think, well, it doesn't really matter.

These are all zeros, but it does matter because when we get to implementing the filter now the edge

effects are much, much bigger.

I would say this is unacceptably large, particularly considering that we don't need to have the filter

go all the way out here.

This is useless.

These are pretty much all zeros.

So that's why it's good to have K be as small as it possibly can be given that this kernel is still

tapering down to zero.

Okay.

There's one more thing I want to show you in this video, which is why it's important to normalize the

Gaussian to unit energy.

And that's really all about not dividing by anything here outside this summation function.

So what I'm going to do is comment out this line, rerun this cell.

Now, this plot doesn't change at all because the plotting happened before the normalization.

And now watch what happens when I run the filter.

So the filtered signal is now this black line.

It's kind of the same as this.

It's just that it's scaled all the way up.

So now I'll go back.

This looks really ugly to me.

I want to I want to end this video with a nice picture.

All right.

That's a good way to end this video.

So I hope you feel like you learned something about the Gaussian smoothing filter and how it differs

from the mean smoothing filter.

I encourage you, as always, to spend some time playing around with this code, trying different values

of the full width at half maximum to see how that affects the smoothness of the result.

And I will see you in the next video.


```{python}
## implement the filter

# initialize filtered signal vector
filtsigG = copy.deepcopy(noisy_signal)

# # implement the running mean filter
for i in range(k+1,n-k):
    # each point is the weighted average of k surrounding points
    filtsigG[i] = np.sum( noisy_signal[i-k:i+k+1]*gauss_win )

# plot
plt.plot(time,noisy_signal,'r',label='Original')
plt.plot(time,filtsigG,'k',label='Gaussian-filtered')

plt.xlabel('Time (s)')
plt.ylabel('amp. (a.u.)')
plt.legend()
plt.title('Gaussian smoothing filter')

## for comparison, plot mean smoothing filter

# initialize filtered signal vector
filtsigMean = copy.deepcopy(noisy_signal)

# implement the running mean filter
# note: using mk instead of k to avoid confusion with k above
mk = 20 # filter window is actually mk*2+1
for i in range(mk+1,n-mk-1):
    # each point is the average of k surrounding points
    filtsigMean[i] = np.mean(noisy_signal[i-mk:i+mk+1])

plt.plot(time,filtsigMean,'b',label='Running mean')
plt.legend()
plt.show()
```


---
# VIDEO: Gaussian-smooth a spike time series
---



```python
## generate time series of random spikes

# number of spikes
n = 300

# inter-spike intervals (exponential distribution for bursts)
isi = np.round(np.exp( np.random.randn(n) )*10)

# generate spike time series
spikets = np.zeros(int(sum(isi)))

for i in range(0,n):
    spikets[ int(np.sum(isi[0:i])) ] = 1


# plot
plt.plot(spikets)
plt.xlabel('Time (a.u.)')
plt.show()
```


```python
## create and implement Gaussian window

# full-width half-maximum: the key Gaussian parameter
fwhm = 25 # in points

# normalized time vector in ms
k = 100;
gtime = np.arange(-k,k+1)

# create Gaussian window
gauswin = np.exp( -(4*np.log(2)*gtime**2) / fwhm**2 )
gauswin = gauswin / np.sum(gauswin)

# initialize filtered signal vector
filtsigG = np.zeros(len(spikets))

# implement the weighted running mean filter
for i in range(k+1,len(spikets)-k):
    filtsigG[i] = np.sum( spikets[i-k:i+k+1]*gauswin )


# plot the filtered signal (spike probability density)
plt.plot(spikets,'b',label='spikes')
plt.plot(filtsigG,'r',label='spike p.d.')
plt.legend()
plt.title('Spikes and spike probability density')
plt.show()
```


---
# VIDEO: Denoising via TKEO
---



```python
# import data
emgdata = sio.loadmat('TimeSeriesDenoising/emg4TKEO.mat')

# extract needed variables
emgtime = emgdata['emgtime'][0]
emg  = emgdata['emg'][0]

# initialize filtered signal
emgf = copy.deepcopy(emg)

# the loop version for interpretability
for i in range(1,len(emgf)-1):
    emgf[i] = emg[i]**2 - emg[i-1]*emg[i+1]

# the vectorized version for speed and elegance
emgf2 = copy.deepcopy(emg)
emgf2[1:-1] = emg[1:-1]**2 - emg[0:-2]*emg[2:]

## convert both signals to zscore

# find timepoint zero
time0 = np.argmin(emgtime**2)

# convert original EMG to z-score from time-zero
emgZ = (emg-np.mean(emg[0:time0])) / np.std(emg[0:time0])

# same for filtered EMG energy
emgZf = (emgf-np.mean(emgf[0:time0])) / np.std(emgf[0:time0])


## plot
# plot "raw" (normalized to max.1)
plt.plot(emgtime,emg/np.max(emg),'b',label='EMG')
plt.plot(emgtime,emgf/np.max(emgf),'m',label='TKEO energy')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude or energy')
plt.legend()

plt.show()

# plot zscored
plt.plot(emgtime,emgZ,'b',label='EMG')
plt.plot(emgtime,emgZf,'m',label='TKEO energy')

plt.xlabel('Time (ms)')
plt.ylabel('Zscore relative to pre-stimulus')
plt.legend()
plt.show()
```


---
VIDEO: Median filter to remove spike noise
---



```python
# create signal
n = 2000
signal = np.cumsum(np.random.randn(n))


# proportion of time points to replace with noise
propnoise = .05

# find noise points
noisepnts = np.random.permutation(n)
noisepnts = noisepnts[0:int(n*propnoise)]

# generate signal and replace points with noise
signal[noisepnts] = 50+np.random.rand(len(noisepnts))*100


# use hist to pick threshold
plt.hist(signal,100)
plt.show()

# visual-picked threshold
threshold = 40


# find data values above the threshold
suprathresh = np.where( signal>threshold )[0]

# initialize filtered signal
filtsig = copy.deepcopy(noisy_signal)

# loop through suprathreshold points and set to median of k
k = 20 # actual window is k*2+1
for ti in range(len(suprathresh)):
    
    # lower and upper bounds
    lowbnd = np.max((0,suprathresh[ti]-k))
    uppbnd = np.min((suprathresh[ti]+k+1,n))
    
    # compute median of surrounding points
    filtsig[suprathresh[ti]] = np.median(signal[lowbnd:uppbnd])

# plot
plt.plot(range(0,n),signal, range(0,n),filtsig)
plt.show()

```


---
# VIDEO: Remove linear trend
---



```python
# create signal with linear trend imposed
n = 2000
trended_signal = np.cumsum(np.random.randn(n)) + np.linspace(-30,30,n)

# linear detrending
detsignal = scipy.signal.detrend(trended_signal)

# get means
omean = np.mean(trended_signal) # original mean
dmean = np.mean(detsignal) # detrended mean

# plot signal and detrended signal
plt.plot(range(0,n),signal,label='Original, mean=%d' %omean)
plt.plot(range(0,n),detsignal,label='Detrended, mean=%d' %dmean)

plt.legend()
plt.show()
```


---
# VIDEO: Remove nonlinear trend with polynomials
---



```python
## polynomial intuition

order = 2
x = np.linspace(-15,15,100)

y = np.zeros(len(x))

for i in range(order+1):
    y = y + np.random.randn(1)*x**i

plt.plot(x,y)
plt.title('Order-%d polynomial' %order)
plt.show()

```


```python
## generate signal with slow polynomial artifact

n = 10000
t = range(n)
k = 10 # number of poles for random amplitudes

slowdrift = np.interp(np.linspace(1,k,n),np.arange(0,k),100*np.random.randn(k))
signal = slowdrift + 20*np.random.randn(n)

# plot
plt.plot(t,signal)
plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude')
plt.show()
```


```python
## fit a 3-order polynomial

# polynomial fit (returns coefficients)
p = np.polyfit(t,signal,3)

# predicted data is evaluation of polynomial
yHat = np.polyval(p,t)

# compute residual (the cleaned signal)
residual = signal - yHat


# now plot the fit (the function that will be removed)
plt.plot(t,signal,'b',label='Original')
plt.plot(t,yHat,'r',label='Polyfit')
plt.plot(t,residual,'k',label='Filtered signal')

plt.legend()
plt.show()
```


```python
## Bayes information criterion to find optimal order

# possible orders
orders = range(5,40)

# sum of squared errors (sse is reserved!)
sse1 = np.zeros(len(orders))

# loop through orders
for ri in range(len(orders)):
    
    # compute polynomial (fitting time series)
    yHat = np.polyval(np.polyfit(t,signal,orders[ri]),t)
    
    # compute fit of model to data (sum of squared errors)
    sse1[ri] = np.sum( (yHat-signal)**2 )/n


# Bayes information criterion
bic = n*np.log(sse1) + orders*np.log(n)

# best parameter has lowest BIC
bestP = min(bic)
idx = np.argmin(bic)

# plot the BIC
plt.plot(orders,bic,'ks-')
plt.plot(orders[idx],bestP,'ro')
plt.xlabel('Polynomial order')
plt.ylabel('Bayes information criterion')
plt.show()
```


```python
## now repeat filter for best (smallest) BIC

# polynomial fit
polycoefs = np.polyfit(t,signal,orders[idx])

# estimated data based on the coefficients
yHat = np.polyval(polycoefs,t)

# filtered signal is residual
filtsig = signal - yHat


## plotting
plt.plot(t,signal,'b',label='Original')
plt.plot(t,yHat,'r',label='Polynomial fit')
plt.plot(t,filtsig,'k',label='Filtered')

plt.xlabel('Time (a.u.)')
plt.ylabel('Amplitude')
plt.legend()
plt.show()
```


---
# VIDEO: Averaging multiple repetitions (time-synchronous averaging)
---



```python
## simulate data

# create event (derivative of Gaussian)
k = 100 # duration of event in time points
event = np.diff(np.exp( -np.linspace(-2,2,k+1)**2 ))
event = event/np.max(event) # normalize to max=1

# event onset times
Nevents = 30
onsettimes = np.random.permutation(10000-k)
onsettimes = onsettimes[0:Nevents]

# put event into data
data = np.zeros(10000)
for ei in range(Nevents):
    data[onsettimes[ei]:onsettimes[ei]+k] = event

# add noise
data = data + .5*np.random.randn(len(data))

# plot data
plt.subplot(211)
plt.plot(data)

# plot one event
plt.subplot(212)
plt.plot(range(k), data[onsettimes[3]:onsettimes[3]+k])
plt.plot(range(k), event)
plt.show()
```


```python
## extract all events into a matrix

datamatrix = np.zeros((Nevents,k))

for ei in range(Nevents):
    datamatrix[ei,:] = data[onsettimes[ei]:onsettimes[ei]+k]

plt.imshow(datamatrix)
plt.xlabel('Time')
plt.ylabel('Event number')
plt.title('All events')
plt.show()

plt.plot(range(0,k),np.mean(datamatrix,axis=0),label='Averaged')
plt.plot(range(0,k),event,label='Ground-truth')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.legend()
plt.title('Average events')
plt.show()
```


---
# VIDEO: Remove artifact via least-squares template-matching
---



```python
# load dataset
matdat = sio.loadmat('templateProjection.mat')
EEGdat = matdat['EEGdat']
eyedat = matdat['eyedat']
timevec = matdat['timevec'][0]
MN = np.shape(EEGdat) # matrix sizes

# initialize residual data
resdat = np.zeros(np.shape(EEGdat))


# loop over trials
for triali in range(MN[1]):
    
    # build the least-squares model as intercept and EOG from this trial
    X = np.column_stack((np.ones(MN[0]),eyedat[:,triali]))
    
    # compute regression coefficients for EEG channel
    b = np.linalg.solve(X.T@X,X.T@EEGdat[:,triali])
    
    # predicted data
    yHat = X@b
    
    # new data are the residuals after projecting out the best EKG fit
    resdat[:,triali] = EEGdat[:,triali] - yHat
```


```python
### plotting

# trial averages
plt.plot(timevec,np.mean(eyedat,axis=1),label='EOG')
plt.plot(timevec,np.mean(EEGdat,axis=1),label='EEG')
plt.plot(timevec,np.mean(resdat,1),label='Residual')

plt.xlabel('Time (ms)')
plt.legend()
plt.show()
```


```python
# show all trials in a map
clim = [-1,1]*20

plt.subplot(131)
plt.imshow(eyedat.T)
plt.title('EOG')


plt.subplot(132)
plt.imshow(EEGdat.T)
plt.title('EOG')


plt.subplot(133)
plt.imshow(resdat.T)
plt.title('Residual')

plt.tight_layout()
plt.show()
```


```python

```
