# Crash course on the fourier transform

The Fourier transform is a pretty wide topic. It is used in many  fields, such as mathematics, statistics, physics or engineering.

![FourierTransform](sigprocMXC_02_spectral/fourierTransformClear.png){#fig-transform-clear}

The idea of the Fourier transform is to transform the data from time domain to
frequency domain, and vice versa (inverse transform). @fig-transform-clear
panel A1 shows a signal in atime domain with a duration of two seconds.
Panel A2 shows the same signal in the frequency domain, with a single peak at
the three Hertz mark, as there are three cycles per second. The peak height is
one unit energy (amplitude), and 0 in any other point, as there are no other
frequencies. The signal in panel B1 has a peak at 8 Hertz (panel B2), with a
smaller amplitude (0.5 in both panels). Panel C1 shows the sum of these two
signals. However, in the time domain plot is not as easy to interpret as in the
other two examples, whereas the frequency domain clearly shows two distinct
spectral components with different frequencies and amplitudes. Frequency domain
visualization becomes even more useful if you start adding noise, whereas the
plot would be unreadable in the time domain
(@fig-transform-noisy contains 4 components with *broadband noise*).

![FourierTransform](sigprocMXC_02_spectral/fourierTransformNoisy.png){#fig-transform-noisy}

If a signal has certain features (sine-like repeating patterns)

The point is that there is the same information in both domains, but it is
easier to interpret in the frequency domain. Sine-like signals are particularly
amenable to this type of analysis, whereas other may be more difficult to
understand.

## How does the Fourier transform work?

How do you get from the time domain into the frequency domain using the
Fourier transform? On layman terms, you start off with a signal in the time
domain, and then you take a sine wave that you want to match against the signal.
The similarity between the signal and the wave is estimated with the
*dot product*. The result is a number indicating how much presence the wave
has within the signal. If we repeat this for waves with different frequencies,
we can make a frequency domain plot, where each of the bars is the similarity
of a wave obtained from the dot product (Fourier coefficients). The full plot
is called an amplitude or power spectrum, depending on wether the Fourier
coefficients have been squared or not. This procedure is called the forward
Fourier transform (from time domain to frequency domain).

Inverse Fourier transform, works in a similar way, allowing to transform data
in the frequency domain back to the time domain. It works by taking the Fourier coefficients, mapping them back onto pure sine waves and then summing them all
together. The usefulness of this technique has to do with some shortcuts that
people use in signal processing that are provided by the convolution theorem.

## Uses

There are two major uses of the Fourier Transform. One is making signals easier
to understand (spectral analysis), getting insights that would more difficult
to extract from the time domain. The other reason is because signal processing
operations in the frequency domain tend to be conceptually easier and faster
to implement on computers than the equivalent computations on the time domain.
For example filtering (convolution).

# Frequency resolution

Frequency resolution is the distance between any two successive frequency bins.
It is controlled by the sampling rates and of the number of time points in the
signal. The sampling rate leads to an important number in signal processing
and spectral analysis, which is called the Nyquist frequency.

The Nyquist frequency is very simply just half of the sampling rate. So if
you measure your data with a sampling rate of 1024Hz, then the Nyquist
frequency is 512Hz.

Now when you apply the forward Fourier transform on a signal the frequencies
you extract range from 0 (DC point), up to the Nyquist frequency, which is one
half of the sampling rate. The number of points you get between these two 
is a direct result of the number of time points in the signal.

* If you only have a few time points in the signal, then you're going to get a relatively
sparse frequency resolution.

* If you have more time points in the signal for the same sampling rate (a longer signal), then you're going to more data points between Zero and Nyquist (better frequency resolution).

If you want to increase the frequency resolution to get more frequencies out
of the signal, then you either need to have have more time points in the signal,
or you can do something called zero padding, which means adding more zeros to
the end of the signal, and that effectively makes the signal longer without
adding any new information into the signal.

# Fourier transform for spectral analyses

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat, wavfile
import scipy.fftpack
import scipy.signal
import copy
```

In this video, we will go through some examples of the Fourier transform.
Some examples are based on simulated data, which can be very useful, as the
user can change the input signals (e.g., noise amount) with a known ground
truth.

We are going to simulate a signal with a sampling rate of 1234Hz with a length
of two seconds. In other words, 1234 time points per second per the definition
of the sampling rate in Hertz.

The time vector goes from 0 to the number of points minus one divided by the
sampling rate. Just linear indexing normalized by the sampling rate. Note this
vector does not go up to exactly, exactly two seconds. This is caused
by starting off at 0, if we started at one it would reach exactly two seconds.

```{python}
## Generate a multispectral noisy signal

# simulation parameters
srate = 1234 # in Hz
npnts = srate*2 # 2 seconds
time  = np.arange(0,npnts)/srate

# frequencies to include
frex  = [ 12,18,30 ]
```

If you go from zero up to the number of points, this is going to be two seconds
plus one sampling rate. Now, that's not really wrong per se, but it's
technically incorrect if you want to get two seconds of data.

So I'm going to generate a multi-spectral signal with three frequencies: 12Hz,
18Hz and 30Hz. First we initialize the signal to be just zeros plus noise.
Then we loop over the frequencies adding sine waves to the noise 
($\sin \left( 2 \pi f t \right)$). The indices are used as amplitudesS.

```{python}
signal = np.zeros(len(time))

# loop over frequencies to create signal
for fi in range(0,len(frex)):
    signal = signal + (fi+1)*np.sin(2*np.pi*frex[fi]*time)

# add some noise
signal = signal + np.random.randn(len(signal))
```

Once the signal has been generated, we extract the coefficients using the fast
Fourier transform (FFT). Then we use the absolute value (`abs`) to get the
magnitude from the coefficients, which is the amplitude.

If we intend to retrieve the original units of the signal, you do need to
normalize this data by multiplying by 2 and then divide by the number of time
points in the signal. However, if you are just interested in the shape of the
spectrum, you don't need either of these two.

The frequency vector is $1 + (n/2)$ (half of the time points) frequency bins
ranging between $0$ and Nyquist (half of the sampling rate).

```{python}
# amplitude spectrum via Fourier transform
signalX = scipy.fftpack.fft(signal)
signalAmp = 2*np.abs(signalX)/npnts

# vector of frequencies in Hz
hz = np.linspace(0,srate/2,int(np.floor(npnts/2)+1))

# Undo the transformation
remade = np.real(scipy.fftpack.ifft(signalX))
```

```{python}
## plots

plt.plot(time,signal, lw=1, label='Original')
plt.scatter(time, remade, s=4,c='k', lw=1, zorder=999,label='IFFT reconstructed')

plt.xlim(0.9, 1.1)
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Time domain')
plt.legend()
plt.show()

plt.stem(hz,signalAmp[0:len(hz)],'k')
plt.xlim([0,np.max(frex)*3])
plt.xlabel('Frequency (Hz)')
plt.ylabel('Amplitude')
plt.title('Frequency domain')
plt.show()
```

Then we plot the time domain signal and the amplitude spectrum of the signal.
You can clearly see that there are some rhythmicity components to this signal
and also a little bit of noise added here. The amplitude spectrum shows the
three peaks with the specified frequencies ($x$) and amplitudes ($y$) which
stand out from a background of uniformly distributed random noise (in the 
freq. domain; broadband noise?).

If we take the inverse Fourier transform (IFT) and we plot it in the time
domain we can see the original and the reconstructed signal overlap perfectly:
we can go into the frequency domain through the Fourier transform and then
apply the inverse Fourier transform and you get back the original signal
flawlessly.

Note if the signal is noisy enough, the components will not be clear on the
frequency domain.

If we increase the amplitude of the noise by multiplying by $3$ the signal 
remains understandable on the frequency domain. If we multiply by $6$, $16$,
$20$... the peaks stand less and less, with some becoming larger than the peaks
we introduced in the synthetic signal.

## Another use

I want to show you another application of the Fourier transform.

This is with real data that I downloaded from Trends Google.com.

And basically I searched trends at Google.com for the search term signal processing.

So the question is how often are people searching the internet using Google for signal processing?

And that result gives you all of these numbers.

So these are the numbers that I downloaded from this result.

And these are normalized numbers and they go per week.

So on this week there was 69, not 69 searches, but a normalized search volume of 69.

And then the next week, more people were searching for signal processing.

The next week more people and then about the same and so on.

So it's one per week over a period of five years.

So here I compute the search power.

So this is the power of this searching rhythmicity.

So I take the fast Fourier transform of the data and then I take the magnitude using the ABS function.

And here I'm squaring it.

So this is the amplitude spectrum.

You can square the amplitude spectrum to get the power spectrum.

So and then here I'm just plotting the search data and then the power spectrum of the search data.

So here you see search volume for each week.

So each dot corresponds to a week and this is around five years here.

So there's a couple of interesting things that you see.

First of all, it looks like it's going a little bit down gently over time.

Perhaps signal processing is less and less interesting over time, or maybe more and more people are

learning signal processing to such a large extent that they no longer need to search the internet for

signal processing.

Anyway, Whatever the reason, there is this general trend to go downwards over time.

And it also maybe looks like there's some rhythmicity in here.

But again, this is kind of hard to see because there's also these local fluctuations and maybe some

of this is noise or weird unaccounted for variations.

So now looking at the frequency domain plot, this looks a little strange at first, perhaps, and maybe

a little disappointing.

It kind of seems like nothing is happening at any frequency.

So there's no rhythmicity anywhere in this search volume?

Actually, that's not the case at all.

What's really happening is that this plot is highly skewed by this one data point that is huge and this

corresponds to zero frequency or the DC offset.

So the DC or zero frequency component simply captures the mean, the average of the signal over the

entire duration of the signal.

So when you look here, you can see that the search volume, as I mentioned, this is normalized units.

So the largest number gets scaled to 100.

Now, it looks like the average is somewhere around 60.

And now notice that we're squaring all the values here.

So 60 squared is 3600.

And that's about where this data point is.

So this really just reflects the average search volume, the fact that the search volume over time is

not zero.

So that's not really very interesting.

So it's often useful in signal processing to eliminate the DC component.

You could do that in the plot just by cutting this value off in the plot, or you can do it by normalizing

the data to get rid of the average value in the data.

And so that's what I'm doing here with this line that's commented out.

So here I'm going to show you two different ways of normalizing data.

One, we can apply Detrending.

So Detrend.

And you already learned about Detrending in the previous section of this course.

So that's going to remove this slow trend line and it's also going to remove the average.

So now let's see what this looks like again.

Okay.

So looking at the frequency domain plot, this is already kind of interesting.

I'm going to get back to the interpretation of these peaks in a few minutes.

But I just wanted to point out that all of this information was present in the previous graph.

It was just hard to see because this variation is relatively small.

Look at these fluctuations relative to that overall mean offset of 3600.

So now the search volume averages at zero over the entire time period and the DC component is zero.

That is its zero power at zero frequency.

However, I don't think this is a really good normalization in this case because there was this downward

going trend and by detrending we've actually removed that trend and that trend is not noise.

That's a valid part of the signal.

That's something important to know about the signal.

So I don't think that Detrending is the right normalization procedure in this case.

Instead, I think a better approach is simply to remove the mean.

So let me get back to this one.

So now you see the trend going down again.

So now what I'm going to do is.

Means center the data, which means just subtracting the mean.

So I say search data equals search data minus mean search data.

Okay.

And now I'm going to try this again.

And now you see that the average is now zero.

So the DC component is gone.

But we still retain this slow drift going down, which is good.

So we haven't really adjusted these long term trends in search volume.

Okay.

Now I want to interpret this plot a little bit.

This power spectrum, as I mentioned, these data are all in weeks.

These are search volumes per week.

So I'm setting up the frequencies vector to go from zero all the way up to 52 in N units.

So this looks a little bit different.

So n corresponds to the length of the search data vector.

So this form looks a little bit different from what I showed up here because here I said n divided by

two plus one.

This is actually just a plotting trick to go all the way up to N because now I don't need to cut off

the Fourier coefficients half way through, which you can see I do here.

So this means that we can interpret these numbers in terms of a fraction of a year.

So this large peak at two means that every 26 weeks, so every one half of 52, which means twice a

year, there is a peak in the search volume for signal processing.

And that's this trend that you see here.

So this is 52 weeks, so this would be a year.

So it's like this and then like this.

So this kind of trend here is what this two hertz sorry, two times per year spectral component is reflecting.

So my guess is that this corresponds to the semester schedule where twice a year people are starting

to learn about signal processing in their engineering course or perhaps a statistics or data analysis

course, and they start searching the internet for signal processing information.

And then there's also a peak at one hertz.

And that is reflected by this larger trend here.

And I guess there's two effects.

So there's one per semester and then there's also an annual increase and decrease in search volume for

signal processing.

Now, this term here, this is the last thing I want to discuss in this video.

If I just showed you this graph, this power spectrum, you would probably be tempted to interpret this

as a very low frequency peak, as a low frequency fluctuation in the signal.

However, in this case, that's really not what's going on.

What's going on is that this slow linear trend, this decrease requires a lot of energy at a kind of

one over F shape in the frequency spectrum.

So that's basically these two coefficients here.

But this first coefficient I've eliminated by mean centering the data.

So I've artificially removed the DC component, which means that in the original data Time series,

this component wasn't really zero.

You saw it before, it was like around 3600.

So this is not actually a real genuine peak in the spectrum.

This actually reflects the steep drop off from zero hertz.

That was capturing this linear decline in search volume.

So I hope you feel like you've gained a little bit of intuition into the Fourier transform.

I encourage you to spend some time with this code and you can change these parameters.

You can change the amplitudes, you can change the amount of noise and try and get a richer intuition

for the relationship between information in the time domain and information in the frequency domain.


```python
## example with real data

# data downloaded from https://trends.google.com/trends/explore?date=today%205-y&geo=US&q=signal%20processing
searchdata = [69,77,87,86,87,71,70,92,83,73,76,78,56,75,68,60,30,44,58,69,82,76,73,60,71,86,72,55,56,65,73,71,71,71,62,65,57,54,54,60,49,59,58,46,50,62,60,65,67,60,70,89,78,94,86,80,81,73,100,95,78,75,64,80,53,81,73,66,26,44,70,85,81,91,85,79,77,80,68,67,51,78,85,76,72,87,65,59,60,64,56,52,71,77,53,53,49,57,61,42,58,65,67,93,88,83,89,60,79,72,79,69,78,85,72,85,51,73,73,52,41,27,44,68,77,71,49,63,72,73,60,68,63,55,50,56,58,74,51,62,52,47,46,38,45,48,44,46,46,51,38,44,39,47,42,55,52,68,56,59,69,61,51,61,65,61,47,59,47,55,57,48,43,35,41,55,50,76,56,60,59,62,56,58,60,58,61,69,65,52,55,64,42,42,54,46,47,52,54,44,31,51,46,42,40,51,60,53,64,58,63,52,53,51,56,65,65,61,61,62,44,51,54,51,42,34,42,33,55,67,57,62,55,52,48,50,48,49,52,53,54,55,48,51,57,46,45,41,55,44,34,40,38,41,31,41,41,40,53,35,31]
N = len(searchdata)

# possible normalizations...
searchdata = searchdata - np.mean(searchdata)

# power
searchpow = np.abs( scipy.fftpack.fft( searchdata )/N )**2
hz = np.linspace(0,52,N)

plt.plot(searchdata,'ko-')
plt.xlabel('Time (weeks)')
plt.ylabel('Search volume')
plt.show()

plt.plot(hz,searchpow,'ms-')
plt.xlabel('Frequency (norm.)')
plt.ylabel('Search power')
plt.xlim([0,12])
plt.show()
```


---
# VIDEO: Welch's method
---



```python
# load data and extract
matdat  = loadmat('EEGrestingState.mat')
eegdata = matdat['eegdata'][0]
srate   = matdat['srate'][0]

# time vector
N = len(eegdata)
timevec = np.arange(0,N)/srate

# plot the data
plt.plot(timevec,eegdata,'k')
plt.xlabel('Time (seconds)')
plt.ylabel('Voltage (\muV)')
plt.show()
```


```python
## one big FFT (not Welch's method)

# "static" FFT over entire period, for comparison with Welch
eegpow = np.abs( scipy.fftpack.fft(eegdata)/N )**2
hz = np.linspace(0,srate/2,int(np.floor(N/2)+1))

```


```python
## "manual" Welch's method

# window length in seconds*srate
winlength = int( 1*srate )

# number of points of overlap
nOverlap = np.round(srate/2)

# window onset times
winonsets = np.arange(0,int(N-winlength),int(winlength-nOverlap))

# note: different-length signal needs a different-length Hz vector
hzW = np.linspace(0,srate/2,int(floor(winlength/2)+1))

# Hann window
hannw = .5 - np.cos(2*np.pi*np.linspace(0,1,int(winlength)))/2

# initialize the power matrix (windows x frequencies)
eegpowW = np.zeros(len(hzW))

# loop over frequencies
for wi in range(0,len(winonsets)):
    
    # get a chunk of data from this time window
    datachunk = eegdata[ winonsets[wi]:winonsets[wi]+winlength ]
    
    # apply Hann taper to data
    datachunk = datachunk * hannw
    
    # compute its power
    tmppow = np.abs(scipy.fftpack.fft(datachunk)/winlength)**2
    
    # enter into matrix
    eegpowW = eegpowW  + tmppow[0:len(hzW)]

# divide by N
eegpowW = eegpowW / len(winonsets)


# plotting
plt.plot(hz,eegpow[0:len(hz)],'k',label='Static FFT')
plt.plot(hzW,eegpowW/10,'r',label='Welch''s method')
plt.xlim([0,40])
plt.xlabel('Frequency (Hz)')
plt.legend()
plt.show()
```


```python
## Python's welch

# create Hann window
winsize = int( 2*srate ) # 2-second window
hannw = .5 - np.cos(2*pi*linspace(0,1,winsize))/2

# number of FFT points (frequency resolution)
nfft = srate*100

f, welchpow = scipy.signal.welch(eegdata,fs=srate,window=hannw,nperseg=winsize,noverlap=winsize/4,nfft=nfft)

plt.semilogy(f,welchpow)
plt.xlim([0,40])
plt.xlabel('frequency [Hz]')
plt.ylabel('Power')
plt.show()
```


---
# VIDEO: Spectrogram of birdsong
---



```python
## load in birdcall (source: https://www.xeno-canto.org/403881)

fs,bc = scipy.io.wavfile.read('XC403881.wav')


# create a time vector based on the data sampling rate
n = len(bc)
timevec = np.arange(0,n)/fs

# plot the data from the two channels
plt.plot(timevec,bc)
plt.xlabel('Time (sec.)')
plt.title('Time domain')
plt.show()

# compute the power spectrum
hz = np.linspace(0,fs/2,int(floor(n/2)+1))
bcpow = np.abs(scipy.fftpack.fft( scipy.signal.detrend(bc[:,0]) )/n)**2

# now plot it
plt.plot(hz,bcpow[0:len(hz)])
plt.xlabel('Frequency (Hz)')
plt.title('Frequency domain')
plt.xlim([0,8000])
plt.show()
```


```python
## time-frequency analysis via spectrogram

frex,time,pwr = scipy.signal.spectrogram(bc[:,0],fs)

plt.pcolormesh(time,frex,pwr,vmin=0,vmax=9)
plt.xlabel('Time (s)'), plt.ylabel('Frequency (Hz)')
plt.show()
```
